%----------------------------------------------------------------------
% MAIN BODY
%----------------------------------------------------------------------

%======================================================================
\chapter{What Meanings Do}
%======================================================================
\renewcommand{\epigraphrule}{0pt}
\setlength{\epigraphwidth}{4.5in}
\epigraph{\textit{In order to say what a meaning is, we may first ask what a meaning does, and then find something that does that.}}{- David Lewis, 1970}

\setlength{\epigraphwidth}{4.5in}
\epigraph{\textit{The point of the theoretical association of meanings with linguistic expressions is to explain the use of those expressions.}}{- Robert Brandom, 2000}

\section{Introduction}

While it is quite clear that Lewis' methodological advice has played a central role in the development of contemporary semantics \citep{LiangPotts:2015}, it is less clear that any consensus has arisen regarding its appropriate application. Common sense suggests that meanings do the job of lending communicative significance to sounds and scribbles that would otherwise be of little interest to us, yet researchers have seen fit to assign a rather diverse set of responsibilities to semantic theory. 

On the one hand, meanings have been invoked to explain how language connects to reality via relations of reference, and to thereby explain how certain configurations of reality are compatible with the truth of some sentences but not others \citep{Speaks:2014,Lewis:1970,Soames:2010}. On the other hand, meanings have also been invoked to explain various facts about linguistic behavior \citep{Wittgenstein:1953,Brandom:1994,Brandom:2000,Horwich:2005,Horwich:1998} and the psychological states of language users \citep{Block:1986,Harman:1982}. Given these diverse aims, one can legitimately wonder whether the various theories of meaning that have been proposed are really theories of the same sort \citep{Block:1986}. And even if they are of the same sort, one can go on to wonder whether it is possible for a single theory to satisfy all of the aims in question \citep{Horwich:1998}. 

My goal here is to argue that ``meanings'' do nothing more or less than codify our implicit expectations regarding certain effects of language use. This idea fits comfortably into a long tradition of work characterizing meaning in terms of use \citep{Sellars:1954,Wittgenstein:1953,Sellars:1953,Harman:1982,Block:1986,Brandom:1994,Brandom:2000,Brandom:2009,Horwich:2005}, but as I hope to show, it provides novel resources for avoiding some of the pitfalls to be found in this earlier work. Specifically, I argue that linguistic expressions can be thought of as informal instruments of prediction that play a role in the folk theory of ``intentional interpretation'' \citep{Dennett:1987,Brandom:1994} that is analogous to the role played by mathematical expressions in scientific theory. When others use particular words and sentences, it allows us to predict what they are likely to do and say next; when we use particular words and sentences, it allows us to have predictable effects on others. The goal of semantic theory, considered as a scientific enterprise, is accordingly to account for \textit{how} language is able to perform this predictive function in the context of intentional interpretation. The key to developing such a theory, on my view, is to formalize the tacit inferential relationships that hold amongst various linguistic expressions and non-linguistic perceptions and actions, and thus explain how tacit expectations are generated on the basis of these relationships. I accordingly develop an inferential role semantics for natural language in the rest of this thesis. 

Three informal observations can help to give my account some initial plausibility. First, the fact that competent speakers are unable to catalog the meanings of the expressions in their language provides \textit{prima facie} evidence that knowledge of meaning is not knowledge of explicit fact. When competent language users invoke morphological variants of the verb ``to mean'', they often do so to to correct or instruct someone regarding the proper usage of a term of interest. For instance, utterances of the following sort are quite familiar: ``That's not what \textit{versatile} means - something is versatile if it can be put to use in a variety of ways''. On the other hand, utterances of the form ``the meaning of \textit{X} is \textit{Y}'', where \textit{X} is a natural language expression and \textit{Y} is a meaning, are patently unfamiliar. As such, there is good reason to suppose that understanding a meaning involves implicitly understanding how to use a linguistic expression \citep{Brandom:1994,Horwich:2005}. Let's Test LatexDiff!

Second, it is quite clear that we form expectations about what other people are likely to do based on what \textit{we} tell them and what \textit{they} tell us. For example, if you tell me that you dislike chocolate, I can reasonably expect you to avoid ordering chocolate cake for dessert. Similarly, if I tell you that I own a dog, then I can reasonably expect you to respond affirmatively if someone else asks you whether I own any pets. These predictions are importantly dependent on the particular expressions present in each utterance. If the word ``chocolate'' is replaced with the compound word ``ice cream'' in the first example, then a new set of expectations are in order - it would no longer be prudent for me to expect that you will avoid chocolate desserts. The point here is that in the absence of minimally correct predictions about what follows from the use of a particular expression, it is implausible to presume an understanding of what an expression means. Overall, language use is fraught with expectations on the part of language users.
 
Third, the existence of these expectations has obvious instrumental value in social contexts. The ability to make successful predictions concerning the state of the world is a prerequisite for survival in most living creatures \citep{clark:2013,Dennett:1987}, and when the state of the world is partly constituted by other living creatures, it is useful to be well informed about what these other creatures are going to do. A language can accordingly be thought of as a social technology that sustains a shared measure of predictive success in the context of collective activity. Put simply, people learn more than they would otherwise learn about what to expect from the world by \textit{talking} to one another.\footnote{Two misunderstandings are worth defending against on this point. First, debates about how language evolved are immaterial in this context; what matters is that it did evolve, and that it possesses properties that help sustain complex forms of cooperative activity. Second, no stand is being taken regarding the conditions that need to be met in order for something to count as a language. The growls one wolf makes towards another wolf can be thought of as a primitive form of linguistic activity, since growls can facilitate expectations (e.g. the second wolf can expect a potential attack from the first) that help sustain collective activity (e.g. the behavior of a wolf pack). Though it is clear that human language is fundamentally different in kind from these more primitive forms of communication \citep{Harley:2014}} It is also worth highlighting how analogous these social predictions are to the kinds of physical predictions that are afforded so naturally by observation. People, for instance, generally expect that an object will fall if nothing is present to hold it up, and that materials of particular types can manipulated in particular ways \citep{Dennett:1987}. Similarly, humans also expect that various uses of language have predictable effects on the thoughts and behaviors of other humans. For example, if one asks the question ``What are car seats made of?'', being given the response ``It's warm outside today'' is almost as surprising as seeing a vase remain upright after being tilted forty-five degrees. A natural extension of these comparisons is the idea that a theory of meaning accounts for certain linguistic phenomena in approximately the same way that a physical theory account for physical phenomena. The goals of theory development in each case include explaining \textit{why} tilted vases and verbal non-sequitors are so surprising. 

On the basis of these informal observations, I think it is reasonable to seriously consider treating meanings as posits of a theory that aims to explain \textit{how and why} we use linguistic expressions in the ways that we do. Attributing a meaning to sentence is not unlike attributing a center of mass to an object or charge to a particle \citep[see][]{Dennett:1987}; in all such cases, the attribution is warranted because it lends explanatory insight into certain phenomena of interest (i.e., phenomena involving sentences, objects, and particles respectively). To lend further support to these claims, I will largely adopt a demonstrative strategy. That is, I will initially assume that meanings ought to be treated this way, and then go on to demonstrate various kinds of valuable explanatory work that can be done given the assumption. Over the course of these demonstrations, I will also clearly identify various aspects of linguistic analysis that do \textit{not} involve meanings directly. For instance, syntax (the study of phrase and sentence structure), morphology (the study of word structure), and phonology (the study of sound structure) all have a role to play in explaining linguistic phenomena, but they do not by this measure count as semantic. Similarly, the study of certain pragmatic features of language use also falls outside of the domain of semantics. For example, it is reasonable to expect that questions are followed by answers over the course of a dialogue, but this expectation is about conversational norms and not about the meaning of any particular linguistic expression.

In the rest of this chapter, I first outline a set of theoretical criteria that follow from my characterization of the explanatory goals of semantic theory. I then go on to provide a critical analysis of how existing approaches to semantics measure up against these criteria. Next, I provide a number of arguments that make up the core of the case in favour of my own approach to semantics, which is most similar in spirit to the work of Dennett (\citeyear{Dennett:1987,Dennett:1991}) and Brandom (\citeyear{Brandom:1994,Brandom:2009,Brandom:2000}). The basic idea is that linguistic expressions mean what they do in virtue of the inferences they license in the context of social practices that involve intentional interpretation (i.e. practices that involve people attributing mental states to one another). I conclude with a discussion of open problems and potential objections; addressing these is the primary burden of subsequent chapters.

\section{Criteria for a Theory of Meaning}

Given the methodological assumption that meaning attributions should explain facts about language use, I propose a set of five criteria for evaluating competing semantic theories. The criteria are:

\begin{enumerate}
  \item Predictive adequacy.
  \item Cognitive plausibility. 
  \item Compositionality.
  \item Intentionality.
  \item Normativity.
\end{enumerate}

\noindent
While these criteria are somewhat distinct from those commonly found in the literature, they should not be entirely surprising. The job of semantics is to provide an account of our shared expectations concerning certain consequences of linguistic activity, but it is quite clear that these consequences are neither simple nor homogeneous. They can involve the mental states and surroundings of language users, and they are as innumerable as the linguistic expressions from which they arise. Moreover, they are the sort of thing that speakers can be mistaken about. The criteria are intended to reflect that the consequences of linguistic activity can be analyzed along these various dimensions. A few comments about each criterion are accordingly in order.

The predictive adequacy criterion is meant to capture the idea that the attribution of a particular meaning to a linguistic expression is warranted only when doing so has explanatory value \citep[see][for related ideas]{Hochstein:2011,Dennett:1987}. Such value, I contend, arises when the meaning attributed to a particular expression gives rise to successful predictions concerning (a) the cognitive states of individuals who use the expression, and (b) the behavior of such individuals. To illustrate with an example, consider the following sentence: ``The boy waited for the pitch and then hit the ball over the fence.'' A competent speaker of English is able to rapidly infer on the basis of this sentence that the boy is likely playing baseball, that he has likely used a bat to hit the ball, and that the phrase ``over the fence'' describes where the ball went after he hit it, rather than the location of the ball when he hit it. The predictive adequacy criterion favours attributing a meaning to this sentence that accounts for the fact that anyone who correctly understands it will provide appropriate answers to questions such as ``Where did the ball go?'' or ``What did the boy likely use to hit the ball?'' Finally, it is worth emphasizing that these expectations are most naturally understood in probabilistic rather than deductive terms: a meaning attribution does not entail the truth of certain counterfactual conditionals concerning linguistic or cognitive phenomena; rather, it favours such conditionals in a violable manner.

The cognitive plausibility criterion is motivated by the idea that if meanings are the sorts of things that are \textit{understood} by language users, then facts about meanings must be consistent with facts about language users' cognitive capacities. An example of a relevant fact about these cognitive capacities is the existence of a memory bottleneck that requires the brain to rapidly compress and recode linguistic input; otherwise, the current input is overwritten by new inputs and lost forever \citep{Christiansen:2015}. If a theory of semantics requires that language users retain highly detailed memories of the linguistic stimuli they encounter, then the theory runs afoul of a well-established result of psycholinguistic research. However, such research can also suggest new hypotheses about the nature of the expectations we form on the basis linguistic activity. For instance, the existence of a memory bottleneck suggests that these expectations are unlikely be so fine-grained as to indicate the exact word transitions or motor behaviors exhibited by another person. The overall point here is that the cognitive plausibility criterion favours the co-development of theories of cognition and language use that are consistent with one another. An obvious example of this kind of co-development can be found in philosophical work that describes the meaning of a sentence in terms of the proposition it expresses, and describes the mental state of a person who understands the sentence in terms of their attitude towards the proposition in question \citep[][Ch. 5]{Dennett:1987}. Whether propositional theories of this sort are successful is, of course, another matter.

The compositionality criterion is motivated by the fact that people are able to understand and produce indefinitely large numbers of linguistic expressions. Many researchers believe that the only way to explain this fact is to suppose that the meanings of complex linguistic expressions are built up from the meanings of simple expressions through the recursive application of a fixed set of syntactic rules \citep{Szabo:2012,Szabo:2013,FodorLepore:1991,Recanati:2012,Unnsteinsson:2014,Pinker:1994,FodorPylyshyn:1988,CappelenLepore:2005}. If meanings were not compositional, it would be difficult to explain how people are able to learn infinitely productive languages from finite amounts of linguistic data. However, it is important to note that the compositionality criterion does not strictly enforce rule-based accounts of the relations between simple and complex meanings. Any non-rule-based account that has the capacity to account for the learnability and use of an infinitely productive language counts as compositional on my view. To illustrate this point, it is worth noting that many non-verbal creatures appear to possess infinitely productive capacities to exploit their physical environments, since these environments can contain indefinitely large numbers of distinct configurations of features. Yet few would argue that non-verbal creatures make use of a system of rules analogous to those proffered by linguists to guide their behavior \citep[cf.][]{FodorPylyshyn:1988}. As such, the compositionality criterion is about whether a theory has sufficient scope to account for the vast number of distinct linguistic expressions, and not about the precise shape the theory takes.\footnote{It might be better to call it a ``generality'' criterion, but I've opted to stick with conventional terminology so as to orient the discussion appropriately.}

The intentionality criterion concerns the idea that linguistic expressions are \textit{about} things and therefore bear important relations to non-linguistic entities such as objects, events, and properties \citep{Speaks:2014,Stanley:2008}. This criterion is accepted by many theorists on essentially stipulative grounds: what \textit{makes} a linguistic expression meaningful is the precisely the fact that it can represent or stand in for something else. Satisfying the intentionality criterion requires giving an account of the facts that determine exactly which things a given linguistic expression is about \citep{Horwich:1998,Brandom:2000}. Unfortunately, philosophers have developed many accounts of this sort in recent decades, none of which are without serious problems \citep{Horwich:1998,Horwich:2005}. In subsequent portions of this thesis, I will use the seemingly intractable nature of these problems to motivate an approach on which the ``aboutness'' of linguistic expressions is characterized in terms of the normative status of certain uses of language. For the time being, however, the intentionality criterion can be sufficiently well motivated by noting that if linguistic expressions were to fail to be about things, it would be difficult to make sense of some very obvious features of communication. To give an example, the sentence ``the saucepan is boiling'' means what it does at least in part because it communicates information \textit{about} a particular saucepan. To give a slightly different example, the phrase ``kick the bucket'' is atypical and idiomatic precisely because it does \textit{not} convey information about a bucket.
 
The normativity criterion, finally, is motivated by the observation that uses of language are subject to evaluations of correctness \citep{Wittgenstein:1953,Brandom:1994,Brandom:2000,Brandom:2009,Sellars:1953}. To give a simple example, describing a sea otter as a type of fish constitutes a misuse of the term ``fish''. Similarly, stating that ``Waterloo is to the east of Toronto'' on the basis of the fact that ``Waterloo is to the west of Toronto'' constitutes of a violation of the norms governing uses of the words ``east'' and ``west'' \citep[i.e. they are not synonyms; see][p. 98]{Brandom:1994}. Specifying the nature of these norms, however, is a complicated matter \citep{Brandom:1994,Kripke:1982}. On one view, they emerge from the interactions between members of a linguistic community, and are irreducible to non-normative facts about this community, since ``ought'' cannot derived from ``is'' \citep{Brandom:1994}. On another view, norms emerge from the demands of instrumental rationality \citep{Horwich:1998}, which can in turn be explained in terms of evolutionary pressures that have shaped us into the kinds of creatures that we are \citep{Dennett:1987,Dennett:2010}. Regardless of whether either view is correct, it is hard to dispute that what an expression means determines how one \textit{ought} to use it; the very fact that meanings are learned (and hence can be learned in error) presupposes as much. As such, a theory of semantics must have something to say about the fact that there are correct and incorrect uses of linguistic expressions.

One interesting feature of these criteria is that they can be construed as more or less general ways of imposing the requirement of predictive adequacy on a theory of meaning. To explain, the cognitive plausibility criterion can be thought of as ruling out theories that are predictively \textit{inadequate} when it comes to characterizing the obvious connections between language and cognition. The compositionality criterion similarly rules out theories that fail to generate good predictions concerning phenomena involving all of the indefinitely large number of possible expressions in a given language. The intentionality criterion rules out theories that are predictively inadequate when it comes to characterizing connections between language use and various non-linguistic perceptions and actions that people employ to interact with the world. The normativity criterion, finally, can be thought of as ruling out theories that assign no degree of impropriety to completely unpredictable forms of linguistic activity. It should accordingly be clear why these criteria have been selected -- they are consistent with the aims of a theory of meaning that is both predictive and explanatory, and therefore fit to play a role in the scientific study of language.  

\section{A Brief History of Semantics}

The approach and the criteria that I have introduced are in many ways at odds with conventional wisdom in philosophy. For this reason, I briefly evaluate two dominant traditions in the study of meaning to illustrate where and why I think conventional wisdom goes awry. The first tradition uses the concepts of truth and reference to define the meanings of various linguistic expressions. These definitions are then used in tandem with certain ancillary hypotheses about social cooperation to derive explanations of various regularities in language use \citep[e.g.][]{Soames:2010,Lewis:1970,Lewis:1975,Davidson:1967,CappelenLepore:2005,Grice:1975}. The second tradition reverses this order of explanation: first, meanings are analyzed in terms of various regularities and social norms that govern language use; then, this analysis is used to derive an explanation of how linguistic expressions take on truth-values and refer to things in the world \citep[e.g.][]{Wittgenstein:1953,Brandom:1994,Brandom:2000,Horwich:2005,Sellars:1953}. Numerous variations on these two basic approaches that can be found in recent work on semantic context-sensitivity \citep{Recanati:2004,CappelenLepore:2005}, and on disputes about the boundary between semantics and pragmatics \citep{Brandom:2000,Recanati:2012}.\footnote{Semantics is typically characterized as the study of linguistic meaning, while pragmatics is typically characterized as the study of language use. Pragmatics is also sometimes characterized as the study of context-sensitivity in language, though this is arguably the same as the first definition, since shifts in context are what make different uses of the same linguistic expression different. By some accounts \citep[e.g.][]{CappelenLepore:2005}, giving a proper theoretical treatment of the distinction between semantics and pragmatics is the central problem in philosophy of language today. Different ways of spelling out this distinction essentially correspond to different attitudes towards the two general kinds of semantic theory just described.}

Using the criteria outlined in the previous section, I analyze some of the dominant theoretical commitments found in each of these two traditions. The first, ``truth-theoretic'' approach to semantics is as close to philosophical orthodoxy as one can get. The second, ``use-theoretic'' approach is more of a minority position \citep{Stanley:2008,Brandom:1994}.

\subsection{Truth-Theoretic Semantics}

The truth-theoretic approach to semantics is largely motivated by the idea that linguistic expressions often represent the world as ``being a certain way'' \citep[][p. 1] {Soames:2010}. For instance, the declarative sentence ``There are two universities in the city of Waterloo'' represents the world as being such that there are two universities in the city of Waterloo. The sentence thus imposes conditions on how the world must be if the sentence is to count as true.

Observations of this sort have led many philosophers to conclude that a sentence's meaning is something that specifies its truth conditions \citep{Soames:2010,Stanley:2008,Lewis:1970,Lewis:1975,Davidson:1967}, and that a word's meaning is something that helps to determine the truth-conditions of the sentences it is a part of \citep{Davidson:1967}. In turn, efforts to provide formal definitions of the truth-conditions for various natural language constructions have become widespread \citep[see][for examples]{Lewis:1970,Carpenter:1997}. The gist of these efforts is that words are taken to refer to objects or sets of objects in some ``domain of discourse,'' while the syntactic rules that combine words into sentences introduce various constraints on how the referents of these words must be related to one another if the sentences in question are to count as true \citep{Stanley:2008}. To give a very simple example, the word ``John'' could refer to the person John, and the word ``walks'' could refer to the set of things that walk. The syntactic rule that combines these words to produce the sentence ``John walks'' requires that the person denoted by ``John'' is included in the set of things denoted by ``walks,'' if the sentence is to count as true. This requirement specifies the sentence's truth conditions, and similar requirements can be generated for a host of other sentences once the semantic consequences of a broader set of syntactic rules have been described.

In recent decades, truth-conditional analyses have been provided for a range of linguistic expressions involving propositional attitude ascriptions, subjunctive conditionals, tense, aspect, and a variety of other things \citep{Carpenter:1997,Soames:2010,Stanley:2008,Speaks:2014}. These achievements have led some to optimistically suggest that it is only a matter of time before a theory that covers the entirety of a natural language is developed \citep[e.g.][]{Soames:2010}. Importantly, this optimism is also sensitive to the fact that such a theory must provide some account of the many linguistic expressions that are prima facie \textit{not} analyzable in truth-theoretic terms. Such expressions include non-declarative sentences (e.g. ``Leave them alone!''), sentences containing words with context-sensitive denotations (e.g. ``I am tired''), and declarative sentences that are not interpreted in accordance with their literal truth-conditions (e.g. ``Everyone stayed home''). In each case, pragmatic accounts of the problematic phenomenon have been developed. Speech act theory, for instance, provides the standard account of how non-declarative sentences are used to express gratitude, issue commands, make promises, ask questions, and so forth \citep{Austin:1962,Searle:1969}. Similarly, Grice's (\citeyear{Grice:1975}) account of conversational cooperation and implicature provides the standard explanation of how sentences are often able to convey something other than what they strictly state to be the case. Context-sensitive expressions, finally, are widely agreed to involve functions that generate a mapping between a sentence's context of use and a specification of its truth-conditions \citep{Soames:2010,Recanati:2012,Recanati:2004,CappelenLepore:2005}. 

Taken together, the above considerations lead to the standard philosophical picture of how language works. First, a semantic theory is used to assign certain entities to the meaningful expressions in a language. In the case of sentences, these entities determine truth-conditions and are called ``propositions'' \citep{Speaks:2014}. In the case of subsentential expressions, the entities are called ``propositional ingredients,''\footnote{Singular terms (i.e. expressions that refer to a unique entity) and predicates (i.e. expressions that denote functions from the referents of singular terms to truth-values) are standard examples of propositional ingredients} and are used in tandem with the rules of syntax to build up the propositions expressed by sentences. In the case of context-sensitive subsentential expressions (e.g. demonstratives), the entities are called ``characters,'' and generate propositional ingredients from contexts of utterance.\footnote{For example, the meaning of the word ``I'' can be analyzed in terms of a character that maps the word onto the person who utters it \citep{Stanley:2008}. So if I utter the sentence ``I am writing'', it expresses the proposition that Peter Blouw is writing. If John Smith utters the same sentence, it expresses the proposition that John Smith is writing. In either case, the proposition expressed has truth-conditions, which accounts for the fact that utterances of ``I am writing'' represent the world as being a particular way.} Second, a pragmatic theory is used to explain the how these propositional entities play a role in various communicative practices that go beyond stating that something is the case. To give one example, a promissory speech act is analyzed partly in terms of a proposition that characterizes the state of affairs that the speaker promises to bring about \citep{KortaPerry:2015}. To give another example, Gricean implicatures are analyzed in terms of \textit{implicated} propositions that are inferred from \textit{stated} propositions on the basis of principles governing cooperative communication \citep{Grice:1975,KortaPerry:2015}. Overall, the standard picture is roughly one in which the job of semantics is to specify the propositions expressed by various sentences in various contexts \citep{Soames:2010,Recanati:2012,CappelenLepore:2005}, while the job of pragmatics is to specify how these propositions are transformed during conversation through various social conventions governing language use. 

How well does this standard picture fare against the criteria introduced in the previous section? Predictive adequacy, first of all, is a complicated matter. On the one hand, truth-conditional theories are typically formulated on the the assumption that languages are abstract objects that can be studied in isolation from empirical facts about language use \citep[][]{Lewis:1970, Jackendoff:2002, Speaks:2014}. The theories are accordingly not designed to make predictions about linguistic behavior, and the structures they describe are seen by many to be prohibitively complex if taken as descriptions of the representations that speakers internalize upon learning a language \citep{Jackendoff:2002,LiangPotts:2015}. On the other hand, many of the pragmatic insights provided by scholars such as Wittgenstein, Austin, and Grice have inspired empirical research on word meanings and linguistic communication \citep{Pinker:1994,Bloom:2001,Harley:2014}. Certain aspects of the truth-theoretic account of interface between syntax and semantics have also been incorporated into mainstream linguistics \citep{SmolenskyLegendre:2006}. But by and large, efforts to produce empirically-oriented linguistic theories have made little use of the set-theoretic structures that philosophers use to define truth conditions \citep{Jackendoff:2002,Harley:2014}. 

With respect to cognitive plausibility, the standard approach to meaning is unquestionably weak. One problem is that the procedures it invokes to build up sentence meanings from word meanings are strictly rule-based and deductive. Linguistic cognition, on the other hand, is widely recognized to be sensitive to probabilistic constraints at every level of analysis from phonology to pragmatics  \citep{SmolenskyLegendre:2006,Christiansen:2015,ChaterManning:2006,Seidenberg:1997}. Another problem is that the entities that are expressed and communicated by sentences (i.e. propositions) are not easily incorporated into psychological theories that describe the cognitive processes involved in linguistic comprehension \citep[][Ch. 5]{Dennett:1987}. A common approach to building a bridge with psychology involves postulating a ``language of thought'' into which linguistic expressions are translated. Token sentences in the language of thought are then taken both to express propositions and to possess causal properties that account for the role that the propositions play in thought and behavior \citep{Fodor:1998}. This general view is increasingly seen to be both empirically inadequate and theoretically outdated. So-called ``propositional attitude psychology'' has long held a dubious position in mainstream cognitive science \citep{Dennett:1987,Churchland:1993}.

The compositionality criterion is a measure on which truth-conditional theories shine. By translating natural languages into formal languages, these theories are able to take advantage of the fact that expressions in formal languages have their contents fixed by their syntactic structure and the contents (i.e. denotations) of their simpler parts. This bottom-up determination of content yields a system in which a fixed set of syntactic rules can be recursively applied to a fixed set of semantic primitives to generate an infinite number of meaningful expressions \citep{Szabo:2013}. The system thus trivially meets the requirement of explaining how people are able to acquire mastery of an infinitely productive language through limited experience --- they simply learn a lexicon and a grammar \citep{Pinker:1994}. 

There is, however, one aspect of the compositionality displayed by truth-conditional theories that poses a problem. Many simple linguistic expressions take on different meanings in different contexts --- recall the ``characters'' that map indexicals onto their referents. This kind of context-sensitivity can become intractably complex if it extends too broadly, since rules specifying how a given context maps an expression onto its referent must be learned in a rote (i.e. non-compositional) manner for each new context \citep{Unnsteinsson:2014}. And indeed, truth-conditional theories are increasingly being extended to include ever larger numbers of such context-sensitive mappings \citep{Recanati:2012,Unnsteinsson:2014}. The merit of these extensions is controversial \citep{CappelenLepore:2005}, but if they end up being necessary, then the case for treating truth-conditional theories as adequately compositional becomes much weaker \citep{Unnsteinsson:2014}. 

The intentionality criterion is another complicated matter. On the standard approach, words and phrase are ``about'' things by virtue of referring to them, but this reference relation is simply stipulated to hold between certain words and certain objects \citep{Stanley:2008}. A number of researchers have attempted to define reference in causal and teleological terms \citep{Speaks:2014,Dennett:1987,Millikan:1989}, but these efforts are both highly controversial \citep{Horwich:2005} and limited in scope: the causes and and teleological functions in question are specified for a few a medium-sized objects like oranges and teapots, while, to adapt complaint from Wittgenstein (\citeyear{Wittgenstein:1953}, p. 5), the rest of the words in the language are just assumed to ``take care of themselves'' along similar lines. Jackendoff (\citeyear{Jackendoff:2002}, pp. 300-03) details the naivety of this extrapolation by itemizing a highly heterogeneous group of entities that can be referred to by ordinary linguistic expressions. Examples include events (e.g. ``the yuletide ball''), geographical locations (e.g. ``Georgia''), social entities (e.g. ``my reputation''), abstract concepts (e.g. ``despondency''), and times (e.g. ``two years from now''). Whatever causal relations we might bear towards such entities, they are certainly not the same as the causal relations we bear towards oranges and teapots. The complexity of these matters suggests that truth-conditional theories do not offer an easy solution to the challenge of explaining intentionality.

The normativity criterion, finally, is handled in a fairly straightforward manner on the truth-conditional approach. The basic idea is that linguistic acts are good largely insofar as they express true claims.\footnote{Of course, other ``goods'' are also relevant. It is, for example, bad to make a true but completely unjustified claim (e.g. a lucky guess), or a true but highly rude claim (e.g. a gratuitous insult)} The question of whether a claim is true, in turn, is typically answered through a consideration of whether the claim corresponds to the way the world is. For instance, the sentence ``There are two universities in the city of Waterloo'' corresponds to the way the world is just in case Waterloo is a city with two universities. Facts about the world are accordingly the dominant constraint on normative assessments of language use. So, to the extent that sense can be made of the notion of a claim ``corresponding to the way the world is'', the normativity criterion is handled fairly naturally on the truth-conditional approach.

Criteria aside, a general complaint about truth-conditional semantics is that it relies heavily on poorly understood and controversial theoretical constructs. To take an obvious example, there is no received view concerning the nature of propositions \citep{Speaks:2014,Soames:2010,Dennett:1987}, and most of the available options arguably introduce more problems than they solve. One common idea is that propositions are sets of possible worlds \citep{Speaks:2014}. Another common idea is that they are structured relations between objects in the world (in which case a proposition concerning a guitar would literally contain the guitar as a constituent part)\citep{Soames:2010}. In the first case, one poorly understood concept (i.e. ``proposition'') is simply defined in terms of another (i.e. ``possible world''). In the second case, it becomes unclear how sentences and mental states are connected to the propositions they express, particularly in cases involving propositional elements that are not concrete objects \citep{Speaks:2014}. Overall, there are good reasons to be wary of granting propositions privileged status in descriptions of mental and linguistic phenomena, especially given that these entities play at best a limited role in neighboring scientific domains like linguistics and psychology.
 
\subsection{Use-Theoretic Semantics}

Use-theoretic approaches to semantics are largely inspired by the later work of Wittgenstein (\citeyear{Wittgenstein:1953}), which repeatedly criticizes the idea that language can be understood in terms of relations of reference that hold between linguistic expressions and states of the world. The crux of the criticism is that referential approaches to meaning provide little insight into much of what makes a language \textit{function as a language}. For example, people use language to provide descriptions, give orders, ask questions, offer thanks, along with a multitude of other things (p. 15), and it is difficult to explain these uses in referential terms. Moreover, the notion that linguistic expressions ``refer'' to things is completely unintelligible in the absence of some prior specification of the practices that give rise to the use of linguistic expressions as a referring devices. Once these practices are examined in detail, it becomes clear that linguistic expressions are more like tools that perform certain social functions than like symbols that connect to states of the world. 

This line of thought culminates in Wittgenstein's suggestion that we think about linguistic expressions in much the same way that we think about the pieces of board games like chess. Each chess piece, like each linguistic expression, performs a certain function and has its use governed by certain rules. And just as we learn chess by learning about the propriety of moving particular pieces in particular ways, so too do we learn a language by learning about the propriety of using particular expressions in particular ways. Communicative interactions, on this model, are ``language-games,'' or cooperative activities governed by a set of implicit conventions. The conventions, in turn, are learned through ostensive instruction, wherein one is initially \textit{shown}, rather than told, how to use various linguistic expressions. 

While Wittgenstein's approach is not without problems,\footnote{It might be more fair to say that Wittgenstein's approach introduced new problems that subsequent scholars have taken great pains to solve. Perhaps most well-known are the problems associated with using rule-following as a model for semantic analysis \citep[see][]{Brandom:1994,Kripke:1982}.} it is nonetheless responsible for two important insights that have shaped almost every subsequent discussion of meaning. The first insight is that linguistic expressions are used in certain ways \textit{because of} what they mean. Any semantic theory that fails to account for this relationship between meaning and use is accordingly inadequate. The second insight is that meaning is a socially instituted, norm-governed phenomenon. So any semantic theory that fails to account for the fact that uses of language can be mistaken or incorrect is also inadequate.

Recent efforts to build on Wittgenstein's work have largely specified the uses of language relevant to a theory of meaning in terms of ``inferential roles'' \citep{Harman:1982,Brandom:2000,Brandom:1994,Block:1986}.\footnote{The terms ``conceptual role'' and ``functional role'' are also often used, but for present purposes I am ignoring the distinctions between these variants. Semantic theories that invoke these conceptual or functional roles are often concerned primarily with psychological explanation and treat the meanings of linguistic expressions as derivative of the meanings of mental states \citep[e.g.][]{Harman:1982}. The reason I elide these distinctions is because all ``role-based'' accounts of meaning rely on the idea that an entity's meaning is determined by certain proprieties governing its behavior, regardless of whether the entity is a private mental state or a public linguistic expression. As Brandom (\citeyear{Brandom:1994}) correctly points out ``talk of functional roles is itself already normative talk'' (p. 16). The notion of an ``inferential role'' makes this normativity evident, since inferences are clearly the sorts of things that are governed by standards of correctness.} The idea here is that understanding the meaning of linguistic expression is tantamount to understanding the propriety of various patterns of inference involving the expression \citep{Brandom:1994}. To give a very simple example, the meaning of the expression ``that's a car'' can be characterized in terms of the fact that it entails ``that's a vehicle'', is entailed by ``that's a sedan'', and is incompatible with ``that's an animal.'' Grasping various entailment and incompatibility relations of this sort is equivalent to grasping an expression's meaning. One advantage of this proposal is that it demarcates semantic phenomena from similar phenomena that are clearly non-semantic \citep{Brandom:2000,Brandom:1994,Brandom:2009}. For instance, a smoke alarm reliably indicates the presence of smoke as well as (or better than) a person who understands the meaning of the word ``smoke.'' Yet the person understands the meaning of ``smoke,'' while the alarm does not. Why? Because the person appreciates the inferential significance of something being smoke: they can infer the likelihood of a nearby fire, a threat to their ability to breath easily, and various smoke-related reactions on the part of other people. All of these inferential relations are lost on the smoke alarm. More importantly, we don't need an account of such expectations to describe or predict the alarm's behavior; we can rely exclusively on physical descriptions of causes and effects involving smoke particles.\footnote{Though it is possible and sometimes useful to introduce semantic notions in such cases \citep{Dennett:1987,Hochstein:2011}. More on this point in the next section.}

While there is no canonical account of the use-theoretic approach to meaning, the above considerations yield a few characteristic themes. First, meanings do the job of explaining language use, regardless of whether the explanations in question invoke use norms \citep{Brandom:2000,Brandom:1994} or use regularities \citep{Horwich:2005,Horwich:1998}. Second, the traditional semantic notions of truth and reference are derived from an antecedent specification of the communicative roles played by truth-and-reference talk \citep{Horwich:1998,Horwich:2005,Brandom:2000,Brandom:1994}. Third, because meanings are defined in relation to one another, it is impossible to understand the meaning of one expression without understanding the meanings of numerous others \citep{Brandom:1994,Brandom:2000}. Fourth, semantic composition is treated in a non-standard way, either by defining the inferential roles of subsentential expressions in terms of their substitutional effects on the inferential roles of sentences \citep{Brandom:2000,Brandom:1994,Block:1986}, or by trivially defining the meaning of a sentence as what results from combining its component parts in accordance with its syntactic structure \citep{Horwich:2005}. Many of these commitments have provoked criticisms from philosophers sympathetic to the truth-conditional approach \citep{Stanley:2008,Speaks:2014}. 

What, then, of the five criteria? Predictive adequacy, first of all, is an area in which the use-theoretic approach shows considerable strength. Largely, this success is due to the fact that the approach is committed to positing meanings on the basis of overt linguistic phenomena (i.e. uses of words and sentences). If the job of semantics is to abstract norms governing \textit{correct} usage from patterns of \textit{actual} usage, then predictions concerning linguistic phenomena naturally follow from these rules or norms if one assumes, reasonably, that people generally act in accordance with them \cite[see][for disscussion of this ``rationality assumption'']{Dennett:1987,Brandom:1994}. This claim is also more than just an ``in principle'' speculation. A considerable amount of work in psychology \citep[e.g.][]{FrankGoodman:2012}, linguistics \citep[e.g.][]{Tomasello:2005,Manning:1999}, and artificial intelligence \citep[e.g.][]{TurneyPantel:2010} has been directly inspired by Wittgenstein's insights concerning the relationship between meaning and use. And more importantly, the statistical methods that characterize this research are now the de-facto standard in contemporary psycholinguistics \citep{Seidenberg:1997,Christiansen:2015}. It's not clear that a similar degree of scientific impact can be attributed to the truth-theoretic approach \citep[cf.][]{Soames:2010}.

Cognitive plausibility is another area in which the use-theoretic approach has significant advantages. For one thing, the approach dovetails nicely with the view that mental states can be identified in terms of the functional relations they bear towards one another, along with perceptual inputs and behavioral outputs. When this view (i.e. functionalism) is paired with a use-theoretic account of the meanings of linguistic expressions, a natural strategy for explaining the relationship between language and thought emerges: one can treat the role of a mental state as an internal model of the role that the state's linguistic correlate plays in public communication \citep{Block:1986}. Uses of the word ``porous'', for instance, exhibit a number of regularities (e.g. pumice stones are regularly \textit{described} as being porous), and thoughts concerning porousness, if they are to count as such, exhibit similar regularities (e.g. pumice stones are regularly \textit{thought of} as being porous). Acquiring a concept, on this model, involves mastering certain proprieties of thought, some of which terminate in actions that display mastery over certain proprieties of language use. A main benefit of the model is that it accommodates the functionalist presuppositions of contemporary cognitive science \citep{Fodor:1998} without introducing a ``language of thought'' and its attendant challenges \citep[see e.g.][]{Dennett:1987}. The use-theoretic approach, in all, avoids the theoretical mystery associated with understanding how thought and language are related to one another if the semantics of thought and language are treated as fundamentally unalike.\footnote{One way in which they might be treated as unalike is if the semantics of mental states are described in relational terms (e.g. states of kind \textit{x} co-vary in some appropriate way with the observable presence of entities of kind \textit{y}), while the semantics of linguistic expressions are described in terms of use. How the use-properties of words derive from the co-variance properties of their mental correlates then emerges as open problem in need of a solution.}

Compositionality is unquestionably a criterion that poses challenges for theorists like Wittgenstein, Sellars, Brandom, and Horwich. It is not at all clear how inferential roles might compose, or even if they are the sorts of things that can compose \citep{FodorLepore:1991}. The most well-developed solution to this problem can arguably be found in Brandom's (\citeyear{Brandom:1994}, Ch. 6) work on substitutional methods for assigning inferential roles to arbitrary linguistic expressions. Roughly, the idea is to group singular terms into equivalence classes with respect to their effects on the inferential roles of the sentences in which they occur, such that swapping one term for another from the same class always produces a pair of sentences that mutually entail one another. Predicates are then organized into partial orders such that swapping one predicate for another creates a pair of sentences that stand in a one-way entailment relation; the sentence containing the ``weaker'' predicate is entailed by the sentence containing the ``stronger'' predicate, but not vice versa. This organization of subsentential expressions into orderings and equivalence classes suffices to confer an inferential role on every possible sentence that can be formed by recombining known subsentential expressions in novel ways. Brandom (\citeyear{Brandom:1994}) argues that this system is as compositional as one could want, but this claim is somewhat controversial. As such, I think it is fair to conclude that the question of whether use-theoretic approaches to meaning can accommodate the compositionality criterion is an open one.

The intentionality criterion is actually less of problem for use theorists than one might initially suppose. To explain, the norms governing a language often involve important relationships between linguistic expressions and certain features of the non-linguistic world. Consider Wittgenstein's (\citeyear{Wittgenstein:1953}) example of a language game in which two workers adopt the following convention: if Builder A calls out ``Slab!'', ``Block!'', ``Pillar!'' or ``Beam!'', then Assistant B retrieves the corresponding object. There is clearly sense in which the builder's words come to be \textit{about} certain things as result of the formation of this convention. A call of ``Beam!'', for example, is about beams because it functions to direct attention and action towards beams alone. The main benefit of this account is that it invokes norms of use to explain \textit{why} particular words refer to particular objects. Truth-conditional theories, on the other hand, often just stipulate the existence of word-object correspondences. As such, usage-based theories of meaning have a natural advantage when it comes to accounting for the ways in which linguistic expressions refer to the non-linguistic world. 

The normativity criterion, finally, is another area of strength for the use-theoretic approach. If linguistic expressions mean what they do in virtue of the role that they play in the social practices of language users, then it becomes possible to understand linguistic norms by examining how people teach each other how to correctly participate in these practices \citep{Brandom:1994,Wittgenstein:1953,Kripke:1982}. It is possible, for instance, examine the processes through which parents teach their children to properly use various words and sentences, and so become competent language users \citep{Brandom:2010,Sellars:1954}. The norms governing a language, on this view, are comparable to the norms governing a simple board game like chess - in either case, people learn to comply with the relevant norms through some mixture of observation, imitation, and responsiveness to correction. However, one important limitation of the ``language game'' metaphor is that it does nothing to explain \textit{why} linguistic communities come to adopt the specific norms that they do. Providing a satisfactory answer to this question probably requires examining the evolutionary history of language use and the sorts of design constraints on linguistic norms that are compatible with a ``working system of communication'' \citep[][p. 54]{Dennett:2010}. But given that similarly broad questions can be asked about the origin of almost any complex human behavior, it is fairly clear that the use-theoretic approach does a good job of accounting for the norms that govern natural language.

Criteria aside, the main challenge facing use-theoretic approaches to meaning is that they are vague and formally imprecise. Talk of social practices and ``language games'' is often seen as metaphorical hand-waving, and as a result, many researchers view the dominant truth-theoretic paradigm as much more rigorous and explanatory. So, if the use-theoretic approach is to win any converts, it likely needs to made much more systematic and formal. 

\section{Meanings as Inferential Roles}

In the beginning of this chapter, I introduced the idea that a theory of meaning is a theory of our implicit expectations regarding certain effects of language use. The preceding discussion helps to situate this idea in relation to the existing literature, but provides no more than a few minimal suggestions as to why it might be correct. As such, the purpose of this section is to spell out the view in much more detail and provide a more complete set of arguments in its favour. The main idea, again, is that the implicit expectations that go along with language use can be codified in terms of ``inferential roles'', which I take to be akin to statistical models that assign probabilities to certain events given the occurrence of certain expressions in the linguistic environment.

I develop this view in three stages. First, I discuss how understanding a linguistic expression requires drawing certain inferences in the context of what Dennett (\citeyear{Dennett:1991,Dennett:1987}) and Brandom  (\citeyear{Brandom:1994,Brandom:2000,Brandom:2009}) call ``intentional interpretation'', or the practice of using mental state attributions to make sense of an agent's behavior. Then, I build on this prior work to describe a novel theory in which a sentence's meaning - its inferential role - derives from its use as a predictive instrument in the context of intentional interpretation. Finally, I provide a number of methodological and empirical arguments in favour of this theory. The result is a basic outline of the inferential role semantics that is developed and defended throughout the rest of this thesis. 

\subsection{Some Background}

To start, it helps to a consider a very basic question: Why do we need a notion of meaning at all? Or put another way, why not restrict our descriptions of linguistic activity to involve only its acoustic, orthographic, or otherwise non-semantic properties? Why not, for instance, speak strictly of causes and effects involving sounds and scribbles?

\subsubsection{The Intentional Stance}

One answer to this question, offered by Dennett (\citeyear{Dennett:1991,Dennett:1987}), is that semantic vocabulary provides a set of tools for accurately predicting phenomena that would otherwise be largely unpredictable. For example, if one attempts to predict human behavior using the non-semantic terminology of physiology or neuroscience, it becomes very difficult to accurately foretell such commonplace events as a parent scolding a child for making a mess, or a shopper haggling over the price of a car. Put simply, it is hard to move from non-semantic descriptions of the visual and auditory stimuli impinging upon a person to high-level predictions concerning their behavior. Yet if one is able to make use of attributions of linguistically specified psychological states, useful (though not infallible) predictions can be generated through no more than a bit of practical reasoning. For instance, if an employer \textit{believes} that their employee is stealing from the cash register, then it follows that the employee is likely to get fired. Similarly, if a man \textit{knows} his spouse is leaving on a week-long trip for work, and \textit{wants} to express that he values their relationship, then he is likely to kiss or hug the spouse goodbye. 

Attributing mental states in this manner in order to generate predictions involves adopting what Dennett calls the ``intentional stance''. Any entity (it need not be a person) whose behavior can be reliably predicted from the perspective of the intentional stance counts as an ``intentional system'' that genuinely possesses beliefs and desires. This claim should not be confused for the idea cognitive systems are internally constituted by identifiable belief and desire states. Rather, being a genuine believer is a matter of objectively satisfying a certain function specification couched in the language of intentional state attributions. How these function specifications are met using the resources available to cognitive systems is a further matter towards which intentional stance theorists are largely neutral. The strategy of treating intentional state attributions as informal calculating devices is sometimes criticized on the grounds that it relativizes facts about psychological states to the predictive abilities of interpreters, but as Dennett notes, ``[t]he decision to adopt the intentional stance is free, but the facts about the success or failure of the stance, were one to adopt it, are perfectly objective'' (\citeyear[][p. 24]{Dennett:1987}).

Given its focus on primarily psychological matters, Dennett's work might appear to be of marginal relevance to theories concerning natural language. But it is also clear that the predictions afforded by the intentional stance are specified in entirely linguistic terms. For example, it is because the employer believes \textit{that the employee is stealing} that a prediction of firing is warranted. If the employer instead believes \textit{that the employee is scrupulous and hardworking}, a prediction of continued employment would instead be more likely. The difference between these two predictions\footnote{Assuming, of course, a static background of further intentional states attributed to the employer. Intentional state attributions are thoroughly holistic in nature \citep{Dennett:1987}.} comes down to nothing more or less than the difference between the sentences falling within the scope of the propositional attitude verb ``believes'' in each case. As such, the intentional stance constitutes ``a calculus for psychological prediction and interpretation'' whose primitives are \textit{linguistic expressions} rather than logical and mathematical expressions of the sort commonly found in more scientific domains (\citeyear[][p. 58]{Dennett:1987}).

The fact that intentional state attributions are so thoroughly linguistic has two interesting consequences. The first is that it is difficult to make sense of the predictions afforded by the intentional stance in the absence of some prior understanding of the linguistic expressions it makes use of. For example, if the sentence ``the employee is stealing'' meant something other than what it does, the predictions afforded by an attribution of a belief in this sentence would be something other than what they are. The second consequence is that it is similarly difficult to make sense of linguistic expressions in the absence of some account of the role that they play in intentional state attributions. Language is clearly \textit{for} communication, but communication is also clearly something that involves people expressing and attributing intentional states to one another. A theory of semantics that has nothing to say about this aspect of communicative activity would accordingly be inadequate. 

\subsubsection{Linguistic Scorekeeping}

One way of thinking about language that incorporates many of Dennett's insights can be found in Robert Brandom's (\citeyear{Brandom:1994}) work on linguistic scorekeeping. Points of overlap between the two authors include commitments to (a) the idea that meaning attributions are akin to function specifications (e.g. \citeauthor{Dennett:1987}, \citeyear{Dennett:1987}, p.~59; \citeauthor{Brandom:1994}, \citeyear{Brandom:1994}, p.~16), and (b) the idea that function specifications are inherently normative (e.g. \citeauthor{Dennett:1987}, \citeyear{Dennett:1987}, p.~49; \citeauthor{Brandom:1994}, \citeyear{Brandom:1994}, p.~16). Where Brandom goes beyond Dennett is in building on these shared commitments to construct a comprehensive account of the semantics of linguistic expressions in terms of the pragmatic significance of the speech acts they are used to perform. Semantics, in his view, ``must answer to pragmatics" (1994, p. 83), which is just another way of saying that meaning attributions have the job of explaining language use. 

There are three key components to Brandom's project.\footnote{\textit{Making It Explicit} is a large book, so many of the more intricate details of Brandom's arguments are omitted here. Some of these details are introduced in later chapters.} The first is the idea that language is best understood within a theoretical framework that describes how various kinds of behavior become normatively significant once they are subject to assessments of propriety by other individuals. For example, the act of taking a pie from a windowsill is normatively significant precisely because others treat the act as either proper or improper depending on who owns the pie. Treating an action as improper might involve sanctioning it somehow, while treating it as proper might involve withholding sanctions or administering rewards of some kind. Once assessments of this sort are in place, actions can confer normative statuses upon the individuals who perform them. One can, for instance, acquire the status of \textit{being a thief} by stealing a pie. 

Linguistic actions are no exception to this general rule - they too can be done either properly or improperly, and are be subject to subtle forms of sanction and reward. For example, nonsense utterances (e.g. ``the the is help the'') are improper, as are utterances involving internally inconsistent sentences (e.g. ``Here's a three-sided square''). Such utterances might be sanctioned by simply being ignored, in which case the penalty of communicative failure is imposed, or by being corrected or questioned, in which case the penalty of communicative inefficiency is imposed. Furthermore, in addition to being subject to standards of propriety, utterances can also determine the propriety of various kinds of further action as well. To explain with an example, the act of uttering ``I like most kinds of dogs'' makes it such that one \text{ought} to acknowledge liking at least some animals if questioned on the matter. The force of this obligation again arises from sanctions imposed by the linguistic community. If one does not use the words ``dog'' and ``animal'' in accordance with the conventions of the community, one will simply not be able to talk to others about dogs and animals. As such, the core idea here is that language use is a communally regulated, norm governed phenomenon. By saying one thing, an individual often acquires a social status that burdens them with commitments to say or do \textit{other} things.

The second component of Brandom's framework is the idea that meanings specify relations that hold amongst the social statuses that are instantiated by linguistic acts. In particular, these relations are described in terms of the inferential connections that hold between various linguistic expressions and intentional states. To explain with an earlier example, if one says ``Waterloo is to the west of Toronto" one acquires a status that commits one to a number of further claims, such as ``Toronto is to the east of Waterloo'' and ``Toronto and Waterloo occupy distinct locations''. This status can be further described in terms of claims one is entitled but not committed to making, such as ``Toronto might be to south of Waterloo'', and in terms of claims one is \textit{not} entitled to make, such as ``Toronto and Waterloo are the same city.'' As such, any commitment to one particular linguistic expression brings with it a complex network of commitments and entitlements to further linguistic expressions. The entirety of this network constitutes the expression's inferential role, which in turn determines the social significance of utterances the expression is involved in. In short, to utter an expression sincerely is to be committed to its inferential consequences and to be open to sanction by other members of the community in the event that these commitments are shirked. 

Given these considerations, the reason that it is appropriate to characterize the inferential roles associated with linguistic expressions as \textit{meanings} is because they explain quite a lot about language use. For one thing, they constitute the ``rules of use'' for a given language, and thus explain the correctness or incorrectness of different forms of linguistic activity. For another thing, inferential roles can be used to generate concrete behavioral predictions if one makes the further assumption that people are generally rational and abide by the norms of their linguistic community. For example, it is reasonably safe to predict that one who claims that England is monarchy will agree with the claim that England has a royal head of state. It is also worth noting that the notion of an inferential role can be extended to include to inferences that relate non-linguistic observations and actions to linguistic expressions, which counters the objection that inferential roles are problematically ``cut off'' from the non-linguistic world \citep[see][pp. 199-271]{Brandom:1994}. And finally, nothing in Brandom's work involves \textit{stipulating} the meanings of linguistic expressions. Rather, it becomes possible to interpret performances as meaningful linguistic performances once it is clear that they are governed by norms describable in terms of relations amongst commitments and entitlements.

The third component in Brandom's framework is the idea is that, as language users, we make sense of one another by tacitly ``keeping score'' of the social statuses that are conferred by our shared linguistic activity. That is, we keep implicit tallies of the commitments and entitlements held by others, and we use these tallies to assess the appropriateness of various utterances at a given stage in a conversation. The approach here is a clear extension of Wittgenstein's (\citeyear{Wittgenstein:1953}) characterization of communicative interactions as ``language games'', and can be described in terms of two basic features. First, at any stage in a language game, the score (i.e. the set of linguistic commitments and entitlements possessed by each interlocutor) determines which ``moves'' are appropriate. For example, a move might be to make a particular assertion or to ask a particular question. Second, the effect of making a move is to change the score, which in turn determines which further moves become appropriate or inappropriate. Overall, the scorekeeping model describes how conversations evolve over time in accordance with a set of linguistic norms. These norms determine how one ought to play ``the game of giving and asking for reasons'' \citep[][p. 23]{Brandom:1994} that constitutes language use, and as such, Brandom's approach provides fundamentally a normative theory of language.

\subsection{The Theory}

The right way to build on this prior work is to selectively amalgamate certain features of Brandom and Dennett's views. My proposal can be thought of as a scorekeeping model (as per Brandom) that emphasizes the predictive utility (as per Dennett) of implicitly keeping track of defeasible expectations associated with the sentences used throughout a conversation. Thus, the notion that we are \textit{predictive scorekeepers} is a useful metaphor for thinking about the idea that a theory of meaning is a formal reconstruction of our implicit expectations regarding certain effects of language use. By uttering one expression, an individual can be expected to assent to or deny a variety of other expressions that are inferentially related to it. As such, treating language users as predictive scorekeepers is consistent with the idea of characterizing meanings in terms of inferential roles, since these roles can be taken to define the constellation of expectations that accompany the usage of particular linguistic expressions. In some ways, this switch to predictive scorekeeping is more a switch in emphasis than in substance, but in other ways, it is quite significant, as the discussion below will illustrate.

Again, a key idea to keep in mind is the analogy between the role that linguistic expressions play in folk theory and the role that mathematical expressions play in scientific theory. The scorekeeping model is clearly a model of how we use linguistic expressions to make sense of one another. It is accordingly a theory \textit{of} our intuitive theories concerning communication. Moreover, the primitives of these intuitive theories are linguistic expressions, since they allow us to informally derive predictions concerning the behavior of others, just as mathematical expressions allow us formally derive predictions concerning the targets of scientific theory. What is needed, then, is a more formal theory of \textit{how} linguistic expressions function as primitives in our implicit theories of the social world. This is the job of semantics.

My answer to the question of how a theory of meaning does this job can be divided into two parts. At the level of pragmatics, a scorekeeping model is used to identify certain conversational regularities that abstract away from the contents of specific sentences. For example, it is reasonable to expect that if one person in a conversation asks a question, the other will provide an answer of some kind. Similarly, if one person makes an assertion, it is reasonable to expect that the other person will either accept this assertion (e.g. ``Thanks for informing me of that.''), challenge it (e.g. ``Do you have any reason to think that?''), or ask a follow-up question (e.g. ``Does that imply this other thing?'') \citep[][Ch. 3]{Brandom:1994}. Certain kinds of assertions also make it reasonable to expect that subsequent assertions are of a particular type \citep{Rohde:2008}. For instance, assertions that describe ongoing events (e.g. ``John was handing a book to Bob'') tend to be followed by assertions that are either explanatory (e.g. ``He wanted Bob to read it'') or elaborative (e.g. ``He reached across the table''). Assertions that describe completed events (e.g. ``John handed a book to Bob''), in contrast, tend to be followed by assertions that introduce new information (e.g. ``He took it and thanked John''). Overall, given that some of the expected effects of language use concern discourse-level transitions between various \textit{types} of linguistic acts including assertions, questions, commands, and challenges, it is natural to carve out a separate niche for more these more ``pragmatic'' predictions on the part of language users. 

At the level of semantics, a model of inferential roles is used to identify conversational regularities that hold amongst specific sentences. For instance, if a language user that asserts a particular sentence (e.g. ``The plane's engines failed mid-flight''), they can be expected to assent to or deny particular further sentences (e.g. ``The plane did not arrive as scheduled''). Similarly, if a language user makes a particular request (e.g. ``Please book a table for two this evening''), they can be expected to assent to or deny particular descriptions of their request (e.g. ``I take it you'd like to make a dinner reservation''). Finally, if a language user makes a specific promise (e.g. ``I'll fly to London to meet you tomorrow''), they can be expected to assent to or deny particular characterizations of their promise (e.g. ``You'll be travelling tomorrow then''). There are subtle differences amongst these examples, but in every case, the initial speaker can be expected to provide specific answers to specific questions merely in virtue of fact that they uttered a specific sentence. The point of assigning inferential roles to sentences, then, is to codify these expectations and then use them to make concrete predictions concerning linguistic behavior. 

The similarities between the view I'm offering and the view offered by Brandom (\citeyear{Brandom:1994}) should at this point be clear. However, one key difference is that linguistic acts, on my view, function to license the attribution of intentional states that are predictive of behavior. An example can make this point clear. Consider a simple conversation in which Alice and Bob are discussing their class on natural language processing. Bob says, ``I could only answer half of the questions on the midterm''. This assertion leads Alice to implicitly attribute a mental state to Bob that licenses a variety of predictions. Because Bob thinks he only answered half the questions on the test, it is reasonable for Alice to expect that he will acknowledge doing poorly, or that he might express an interest in studying more regularly. Alice, with these expectations tacitly guiding her behavior, asks ``Was there a particular topic you had trouble with?'' This question then leads Bob to implicitly attribute a mental state to Alice the specifies her interest in learning more about Bob's academic woes. The attribution of this intentional state licenses the predictions that Alice will be puzzled if Bob suddenly changes the subject, and that Alice will continue the conversation normally if Bob mentions any of the standard topics covered in the course. Whatever Bob's actual response is, it will license a further mental state attribution on the part of Alice, and lead her to form further expectations that guide how she goes on to participate in the conversation. Overall, the point of this example is to illustrate how the attribution of particular inferential roles to particular sentences is warranted precisely insofar as these attributions are predictively adequate in the context of intentional interpretation. Language use involves adopting the intentional stance, and the meanings of linguistic expressions are what determine how the specific predictions afforded by this stance get derived. The goal of semantics is to provide a rigorous account of \textit{how} linguistic practices involving intentional interpretation actually work. 

So, to summarize, my theory treats language use as a social practice regulated by the expectations of language users. Some of these expectations are pragmatic in nature and concern high-level discourse transitions. Other expectations are more semantic in nature, and concern relations amongst the specific sentences that used over the course of a conversation. Together, these expectations determine how a conversation evolves by guiding speakers to alternate between particular kinds of conversational moves (e.g. asking a question vs. answering one) that involve saying particular things (e.g. asking or answering a question with a specific sentence). A theory that properly combines a scorekeeping model of pragmatics with an inferentialist model of semantics thus provides a good foundation for explaining and predicting a wide range of linguistic phenomena. 

\subsection{Methodological Arguments}

A main benefit of this approach is that it commits the theorist to no more and no less than is required to account for available linguistic phenomena. It makes no pre-theoretical commitment to the existence of propositions, possible worlds, or various other entities of dubious metaphysical status. Avoiding such commitments, moreover, has the advantage of reducing one's explanatory burdens. To explain with an example, a truth-conditional theorist needs to first show that propositions can do everything that meanings need to do, and \textit{then} show that propositions themselves can be understood in some non-trivial way. Meeting both of these burdens has proven exceedingly difficult, as was shown in the previous section on the history of semantics. An inferential role semantics, by comparison, appeals to nothing more than relations amongst linguistic expressions, non-linguistic observations, and non-linguistic actions, none of which are metaphysically unusual. These relata are simply part of the natural world, just like pebbles and raindrops. As such, it should be clear that considerations of methodological parsimony favour the inferentialist approach. 

So do considerations of methodological effectiveness. Given that the phenomena targeted by the theory (i.e. features of linguistic activity) are natural phenomena, and that the components of the theory itself (i.e. relations amongst linguistic expressions, etc.) are naturalistic entities or mathematical entities, then there is every reason to treat development of the theory as a scientific enterprise. Doing so has a number of advantages. First, there are generally accepted methods for improving, testing, and challenging scientific theories \citep{GodfreySmith:2002}. Second, measuring progress becomes relatively straightforward: if a change to the theory allows it to account for more of the relevant linguistic data, then an advance has been made. As such, if one adopts an inferentialist approach to semantics, then one obtains all of the familiar epistemological benefits that accompany naturalistic inquiry. This is clearly a good thing.

A helpful way to expand on this argument is to consider the differences between Brandom's (\citeyear{Brandom:1994}) version of scorekeeping and my own. Both are committed to the idea that an expression's meaning determines how one ought to use it, so both are normative theories on some level. Where they differ is in the characterization of this normativity. On Brandom's model, the normative statuses associated with language use are instituted by the assessments and attitudes of a linguistic community. But as Dennett (\citeyear{Dennett:2010}) notes, this treatment of linguistic norms as social constructions raises numerous questions concerning which norms communities tend favour and why. Brandom (\citeyear{Brandom:2010}) is admittedly open to various approaches to answering these questions, but is nonetheless quite resistant to the idea that linguistic norms can be understood in fully naturalistic terms (e.g. \citeauthor{Brandom:1994}, \citeyear{Brandom:1994}, p.~44-46, 290; \citeauthor{Brandom:2000}, \citeyear{Brandom:2000}, p.~26-27). The scorekeeping model is in one sense ``norms all the way down'' \citep[][p. 44]{Brandom:1994}.

I think this resistance to naturalistic explanations is a mistake. Put simply, separating the normative from the natural makes it difficult to fulfill the very explanatory goals that Brandom sets out for himself. Facts about language use are natural facts, and if meanings are to explain these facts, they cannot be isolated to some ``autonomous normative realm'' \citep[][p. 133]{Horwich:2005}. The problems here are threefold. First, it becomes unclear what could count as evidence for or against a particular characterization of the linguistic norms present in a community. Norms that ``go beyond'' the natural facts of language use are in an important sense immune to revision on the basis of these facts, and needless to say, our understanding of such norms would demand a peculiar epistemological status. Second, it is not clear why one should postulate the existence of norms that make no difference to linguistic behavior. Linguistic norms are importantly different from other kinds of norms (e.g. moral norms) in that they are undeniably constituted by communities, in which case norms that have no evidential basis in the activities of these communities are theoretically superfluous. Third, a mystery arises concerning the origins of these norms. They cannot arise by magic \citep{Dennett:2010}, yet if they do not arise from facts about linguistic communities that are describable in naturalistic terms, it is not clear where they \textit{do} come from. 

Arguably, all of these problems are avoided by grounding linguistic norms in predictive utility. The idea here is to invoke a criterion for the attribution of linguistic norms that parallels Dennett's (\citeyear{Dennett:1987}) criterion for the attribution of intentional states: linguistic expressions have certain norms of use insofar as they can be predictively attributed these norms. And since the norms in question can be codified as inferential roles, the attribution of an inferential role to a linguistic expression is similarly done on the basis of predictive utility. So, to adapt a line from Dennett (\citeyear[][p. 29]{Dennett:1987}), all there is for an expression to mean \textit{X} is for the inferential norms codified by \textit{X} to occur ``in the best (most predictive) interpretation'' of the expression's behavior in a linguistic community. This approach clearly makes meaning attributions responsive to evidence (since predictions can be more or less accurate), and it also clearly rules out superfluous meaning attributions that have no interpretative role to play. Moreover, the approach suggests a plausible account of \textit{why} certain linguistic norms are in place: communities construct norms that implement minimal solutions to the kinds of problems that languages solve. To explain, a language sustains complex forms of cooperation and social coordination that would not be possible if the norms governing it did not foster a shared measure of predictive success amongst those who use the language.

The overall methodological case in favour of an inferential role semantics is simple. First, the approach is \textit{parsimonious} in that does not presuppose the need to use possible worlds, propositions, or any other oddities in the development of semantic theory. Second, the approach is \textit{effective} in that it is consistent with the use of scientific methods for semantics. The use of these methods, as the discussion of Brandom illustrates, yields a number of explanatory advantages that are unique to my approach.

\subsection{Evidential Arguments}

Is there any evidence that people comprehend language by drawing inferences and making predictions in the manner I have suggested? Various forms of contemporary research on language acquisition, language processing, and cognition in general suggest that the answer to this question is yes.

A first strand of evidence comes from research on word learning. Children are highly reliant on social cues such as hand gestures, emotional expressions, and eye gaze when learning how to use words, and they exhibit significant linguistic impairments in the absence of ordinary capacities to reason on the basis of such cues \citep[][p. 1099-1100]{Bloom:2001}. For instance, the deficits in ``theory of mind'' that are characteristic of autism go hand-in-hand with substantial linguistic deficits \citep{Bloom:2001,Miller:2006,Harley:2014}. Considerations of this kind have led to theories on which word learning is a consequence of a child's ability to jointly attend to certain aspects of the environment with an adult, and then assign pragmatic significance to certain behaviors with the context of this joint attentional frame \citep{Tomasello:2005,Miller:2006}. 

Support for these theories can be found in a variety of experimental results. For example, Tomasello (\citeyear{Tomasello:2005}) discusses the finding that children engaged in a toy clean-up activity with an adult will fetch toys on the basis of pointing gestures, but not if the gestures come from a second adult who was not initially participating in the activity.\footnote{The similarities to Wittgenstein's (1953) ``Slab'' language-game here are telling.} Similarly, children will use pointing gestures to help adults find objects, and will imitate object-directed actions performed by adults \citep{Tomasello:2001}. Once such gesture-based communicative conventions are in place, children then learn to adopt similar conventions that involve word use. Roughly, a child first determines the pragmatic significance of communicative acts involving a word (e.g. ``Here's a toy!'' and ``Bring me the toy'', accompanied by pointing), and then attempts to ``assign blame'' for some aspect of this significance to the occurrence of the word \citep[][p. 73]{Tomasello:2005}. Eventually, the child will learn that the word ``toy'' can be used in communicative acts to direct attention towards items of a certain kind (i.e. items that are toys). An important conclusion drawn from this research is that it is somewhat misleading to think of word learning in terms of mappings between sounds and objects given the foundational role of shared attention in communication. In simple cases at least, a word is essentially a tool for manipulating the attention of other agents in predictable ways \citep{Tomasello:2001}, and word learning is learning how to use such a tool. 

To appreciate the significance of this research, recall that on the inferentialist model, the meaning of a linguistic expression involving the word ``toy'' would be characterized in terms of implicit expectations concerning its use. This is arguably exactly what children are learning when they learn that the communicative role of the word ``toy'' in an utterance is to direct attention towards a certain kind of object. Put another way, the child is learning what to expect from others who make utterances containing ``toy'', along with what effects he or she can expect to have on others by making similar utterances. What has been learned, if effect, is a simple network of expectations that accompany toy-talk. Empirical research on word learning accordingly fits well with an inferentialist approach to semantics. 

A second strand of evidence comes from research on language acquisition in general. Traditionally, linguists have assumed that learning a language requires inferring a set of abstract grammatical rules from sentences encountered in the speech environment \citep{Harley:2014,Pinker:1994,Seidenberg:1997}, and that the difficulty of this learning problem entails the existence of innate grammatical knowledge \citep{Harley:2014,Seidenberg:1997}. More recent developments, however, suggest that language acquisition is a better thought of as a kind of skill acquisition, wherein the learner develops an ability to \textit{process and use} linguistic expressions correctly \citep{Christiansen:2015,Seidenberg:1997}. One prominent reason for the shift towards this ``learning as skill-acquisition'' model is the explanatory success of probabilistic and neural network models in psycholinguistics \citep{Seidenberg:1997}. To explain with an example, an artificial neural network learns a set of parameters (i.e. connection weights) that approximate a function defined by a set of input-output pairs. These pairs might map words to collections of phonemes during a generation task, or to collections of property concepts during an interpretation task \citep[see, e.g.][]{McClelland:2010}. The important point is that these models learn by doing: they acquire their functional capacities by iteratively reducing some measure of performance error, not by inferring some abstract grammatical structure. Moreover, it is quite clear that what these models are learning to do is perform certain inferences (e.g. inferences from words to collections of properties). 

This research on language acquisition corroborates the inferentialist model in two ways. First, it provides concrete evidence that learning a language is not learning an abstract symbol system consisting of various representations that somehow latch on to particular features of the world. Second, it shows that learning a language involves learning to predict upcoming events in the linguistic environment, since such predictions are needed to cope with the sheer volume of linguistic input. Moreover, when such predictions fail, comprehension suffers. So-called ``garden path'' sentences,\footnote{The classic example of a garden path sentence is ``The horse raced past the barn fell'', for which it is natural to treat ``raced'' as the main verb until the puzzling arrival of ``fell'', which signals that ``raced'' should interpreted as part of a relative clause.} for example, are difficult to understand precisely because they involve continuations that are \textit{unpredictable} given the surrounding context \citep{Christiansen:2015}. Together, these considerations provide support for the idea that linguistic comprehension should not be characterized in terms of ``the turning on of a Cartesian light'' (\citeauthor{Brandom:1994}, \citeyear{Brandom:1994}, p. 120; also qtd. in \citeauthor{Dennett:2010}, \citeyear{Dennett:2010}, p. 53), but rather in terms of a practical ability to draw certain inferences.

A third strand of evidence comes from research on language processing. At the level of dialogue, people tend to anticipate the trajectory of a conversation so as to eliminate pauses between shifts from speaking to listening \citep{Christiansen:2015,Pickering:2013}. A similar kind of anticipation directs people to expect particular kinds of transitions from one sentence to the next \citep{Rohde:2008}. As mentioned, sentences describing ongoing events (e.g. ``'John was handing a book to Bob'') tend to generate expectations of transitions that are either explanatory (e.g. ``He wanted Bob to read it'') or elaborative (e.g. ``He reached across the table''). Sentences describing completed events (e.g. ``John handed a book to Bob''), in contrast, tend to generate expectations of transitions that introduce new information (e.g. ``He took it and thanked John''). These expectations are revealed by how speakers resolve both syntactic ambiguities and lexical ambiguities involving pronouns \citep{Rohde:2008}.\footnote{A more detailed discussion of this research is provided in Chapter 5.}

At the level of individual words and sentences, it is widely thought that linguistic cognition is governed by intuitive theories that place lexical items into explanatory relations with one another \citep{Murphy:1985,MargolisLaurence:1999}. Take the word ``stove'' as an example. To understand what a stove is, a person must grasp that the function of the stove is to heat cookware, that this heating is caused by electric elements, and that the certain switches turn these elements on and off. Or to put things in more familiar terms, in order to acquire the concept of a stove, one must learn to master certain stove-related inferences. So the view that concepts are structured as commonsense theories is basically a psychologically-oriented version of  inferential role semantics \citep{Fodor:1998}. It is convenient, then, that this of view of concepts is supported by such a broad range of empirical evidence. Categorization judgments, for example, are demonstrably influenced by peoples' commonsense understanding of how various objects and properties are related to one another \citep{LinMurphy:1997, Murphy:1985}. Commonsense reasoning also constrains how easily people are able to learn novel categories involving unfamiliar collections of objects \citep{Murphy:1985}. Overall, the idea that inference and prediction are at the core of linguistic cognition is well-supported by contemporary research on concepts. 

A final source of evidence in favour of my view concerns the use of prediction as a general compensatory strategy for successfully dealing with a rapidly changing and complex environment. The idea that brains are ``essentially prediction machines'' \citep[][p. 181]{clark:2013} is increasingly well-supported by research in computational neuroscience that focuses on control-theoretic descriptions of cognitive systems \citep{Eliasmith:2013,Grush:2004,Eliasmith:2010,EliasmithAnderson:2003}. One the central lessons of this research is that effectively controlling a dynamical system as complex as a human body requires generating estimates of future states of the system, and then fine-tuning control signals on the basis of discrepancies between these estimates and what is actually observed \citep{Grush:2004,Pickering:2013,Pickering:2007}. For instance, it is well-established that simple reaching and grasping motions involve the use of ``forward models'' that anticipate the effects of motor commands to help correct for noise and provide ongoing feedback \citep{Pickering:2013,Pickering:2007}. It is also fairly well-established that forward models are involved in both the production and comprehension of speech \citep{Pickering:2013,Pickering:2007}. This is unsurprising given that speech is a controlled behavior in much the same way that walking or reaching is. Overall, given that predictions are needed to successfully control behavior in a rapidly changing environment and that language use is trivially a form of behavior, if follows fairly straightforwardly language use involves prediction. 

\section{The Criteria and the Road Ahead}

The preceding arguments make a reasonable case for the view that language use is fundamentally organized around prediction and inference. What meanings \textit{do}, on this view, is account for the significance that linguistic expressions have for the interpersonal expectations we form on the basis of language use. These expectations can, in turn, be characterized in terms of inferential roles, but doing so gives rise to a number of well-known problems that need to be dealt with \citep{FodorLepore:1991,FodorLepore:2002}.

A first problem is that inferentialist approaches to semantics have yet to be developed using the same degree of technical sophistication as truth-conditional approaches. There is a long history of using tools from mathematical logic to produce truth-conditional analyses of a wide range expression types \citep[e.g.][]{Carpenter:1997}, and the absence of a similar degree of technical sophistication in inferentialist camp is readily apparent. For instance, Dennett (\citeyear{Dennett:1987}) is quick to admit that intentional systems theory is ``woefully informal and unsystematic'' in comparison to approaches that interpret systems in computational or mathematical terms (p. 67). Chapter 2 introduces and expands upon a number of methods from research on natural language processsing to help make the ideas introduced in this chapter both formal and systematic in the required way. Chapter 3 builds on this work by introducing a model that can be used to define the inferential roles of a wide range of sentences. 

A second problem, already discussed to some extent, is that inferential roles are widely seen to be non-compositional \citep{FodorLepore:2002,Fodor:1998,MargolisLaurence:1999}. The inferences that do the explanatory work during intentional interpretation involve sentences, not words, which makes it difficult to assign inferential roles to subsentential expressions and then use these to ``build up'' to the sentential level \citep{Brandom:1994}. In Chapter 4, I argue that the kind of generalization the compositionality criterion is invoked to explain can be achieved through a sort of interpolation between familiar examples of correct inferential transitions. Importantly, this notion of interpolation is derived from an examination of the inferential effects of substituting certain words and phrases in a sentence for others \citep{Brandom:1994}. I argue that the resulting semantics is compositional in every sense that matters.

A third problem concerns the relationship between thought and language. Linguistic expressions are often treated as vehicles for transporting thoughts \citep{Brandom:1994,KortaPerry:2015,Fodor:1998}, but this transportation model is arguably incompatible with an inferential role semantics \citep{Brandom:2010,Brandom:1994}. The difficulty lies in the fact that an inferential role is not something that gets directly communicated by a linguistic expression \citep{Brandom:2010,Brandom:1994}; rather, it is something that determines how people who use a particular linguistic expression ought to or are likely to think and behave. Chapter 5 accordingly develops a view that treats theories of cognitive architecture as theories of how cognitive systems are internally organized so as to satisfy certain function specifications couched in the language of intentional state attributions. 

A fourth problem concerns the relations between truth, reference, and meaning. Sentences are kinds of things that can be true or false, and words are the kinds of things that refer to objects in the world. An explanation of these facts is clearly required to satisfy the intentionality criterion. The challenge, in short, is to characterize truth and reference in primarily inferential terms. I take up this challenge in Chapter 6 by recasting questions about truth conditions and reference relations as questions about the normative statuses of particular linguistic acts. These normative statuses, in turn, are defined in purely inferential terms. A true claim, for instance, is claim that is good in virtue of being inferentially indefeasible. Working out the details of this view leads to an inferential characterization of traditional semantic concepts.

\section{Conclusion}

The purpose of this chapter has been to motivate an approach to semantics based on the idea that meaning attributions function to explain linguistic phenomena. This idea was then made more concrete by characterizing language use in terms of practices of implicit scorekeeping in which speakers rely on linguistic expressions as informal calculating devices to keep track of their expectations of one another. Meanings, in the context of these scorekeeping practices, are inferential roles that determine the expectations speakers attach to one another on the basis of their shared conversational activity. This overall approach was defended on of both methodological and evidential grounds, but a number of open problems remain. The main objective of subsequent chapters is to demonstrate that these problems have plausible solutions, and that an inferential role semantics for natural language can accordingly be used to construct a successful theory of meaning. 

Here's a bunch of additional stuff to test the diff using overleaf. 



