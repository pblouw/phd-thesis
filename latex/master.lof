\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Basic model architecture for learning word embeddings via prediction. A binary input vector, $x$, encodes a collection of one or more words that constitute a context. This input is then passed through a simple neural network that computes a probability distribution over words. Gradient descent is used to learn the weights $W_1$ and $W_2$ so that this probability distribution accurately reflects the likelihood of word co-occurrences in the training corpus. Embeddings for individual words correspond to the rows and columns of the weight matrices. Figure adapted from Rong (\citeyear {Rong:2014}).}}{43}{figure.2.1}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Unfolding a recurrent neural network through time. At each time step, an embedding corresponding to the current word in a sequence (e.g. a sentence) is provided as the input state $x$. The hidden state $h$ and the output state $y$ are then updated. Over time, $h$ and $y$ come to encode information about the entire history of the input sequence.}}{51}{figure.2.2}
\contentsline {figure}{\numberline {2.3}{\ignorespaces A comparison of dependency and constituency tree structured neural network models. In each case, a network layout is constructed from a parse of the input sentence. Constituency-based networks include layers corresponding to phrasal constituents (e.g. noun phrases), while dependency-based networks do not. Variables are labeled for consistency with the naming conventions in equations (\ref {eqn:ct_rnn}), (\ref {eqn:dt_rnn_leaf}), and (\ref {eqn:dt_rnn_nonleaf}), which formally describe how sentence embeddings are produced using each model.}}{53}{figure.2.3}
\contentsline {figure}{\numberline {2.4}{\ignorespaces General model architecture for predicting inferential relationships between pairs of sentences \citep [see][]{Bowman:2015}. The premise and hypothesis sentences are each embedded using one of the models described in Section 2.4 above. The resulting embeddings are concatenated and fed into a simple neural network that performs a three-way classification (entailment, contradiction, or neutral) using a single tanh hidden layer and a softmax output layer.}}{60}{figure.2.4}
\contentsline {figure}{\numberline {2.5}{\ignorespaces A modeled estimate of part of the inferential role for the test sentence ``Some kids are wrestling on an inflatable raft.'' The network is constructed by using a tree-structured model trained on SNLI to predict a class label for each sentence pair connected by a colored arrow.}}{62}{figure.2.5}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Sentence encoding with a dependency-based tree-structured neural network. A dependency parser is used to produce the computational graph for a neural network, which is then used to produce an embedding of a sentence by merging embeddings of individual words. Figure adapted from Socher et al. (\citeyear {Socher:2014}).}}{68}{figure.3.1}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Generating entailments with paired encoder and decoder neural networks. The decoder network computes a probability distribution over words at each node, conditioned on the sentence representation produced by the encoder. The parameters of both the encoder and decoder are trained via backpropogation through structure using error derivatives supplied at each node in the decoding tree. The encoder and decoder trees are dynamically generated for each pair of sentences in the training data. During inference, it is possible to use different decoding trees to generate different entailments from a single sentence encoding.}}{70}{figure.3.2}
\contentsline {figure}{\numberline {3.3}{\ignorespaces A model-generated inferential network around the sentence ``Some kids are wrestling on an inflatable raft''. Each inferential transition is the result of generating a predicted entailment after encoding the sentence at the beginning of each arrow. The entire network is generated starting with only the initial sentence at the center of the diagram, which is drawn from the SNLI test set.}}{76}{figure.3.3}
\contentsline {figure}{\numberline {3.4}{\ignorespaces A model-generated inferential network around the sentence ``A man is outside''. Each inferential transition is the result of generating a predicted entailment after encoding the sentence at the beginning of each arrow. The entire network is generated starting with only the four outermost sentences, which are drawn from the SNLI test set.}}{77}{figure.3.4}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
