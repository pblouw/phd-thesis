%----------------------------------------------------------------------
% MAIN BODY
%----------------------------------------------------------------------

%======================================================================
\chapter{Intentional Interpretation and Cognitive Architecture}
%======================================================================
\renewcommand{\epigraphrule}{0pt}
\setlength{\epigraphwidth}{4.5in}
\epigraph{\textit{It is not that we attribute (or should attribute) beliefs and desires only to things in which we find internal representations, but rather that we find some object for which the intentional strategy works, we endeavor to interpret some of its internal states and processes as representations. What makes some internal feature of a thing a representation could only be its role in regulating the behavior of an intentional system.}}{- Daniel Dennett, 1987}

\section{Introduction}

A common view of the relationship between thought and language, often traced back to Locke \citep{Brandom:1994,KortaPerry:2015}, is that linguistic expressions are like vessels that transport ideas from one mind to another. Speaking involves encoding a thought into a sentence, and understanding what was spoken involves decoding a sentence back into the corresponding thought. A widely appreciated consequence of this model of communication is that the meanings of linguistic expressions are parasitic on the meanings of the mental states they are used to express. Attempts to deal with this consequence have resulted in a substantial literature on the semantics of mental representations \citep{Fodor:1998,Block:1986,Harman:1982}, and as a result, the Lockean ``transportation model'' of communication has achieved something close to default status in contemporary philosophy. 

A challenge for the model is that it requires an account of what is shared by two people who come think or believe the same thing as a result of talking to one another. One proposal is that they share type-identical sentences in a ``language of thought'' whose atomic entities -- symbols -- represent particular features of the world by causally connecting to them in a special way \citep{Fodor:1998}. Other proposals postulate different representational constructs (e.g., prototypes, exemplars, theory-like structures etc.) that similarly connect to particular features of the world. So the answer to the challenge is roughly that successful communication results in two speakers adopting identical representational states, where two states count as identical if they refer to the same thing \citep{Fodor:1998,Horwich:2005}.\footnote{I say ``roughly'' because this description omits various complications that arise when representations \textit{refer} to the same thing, but do not to \textit{mean} the same thing (i.e., so-called ``Frege cases''), and when one tries to answer the question of what sentences (as opposed to words) refer to. Considerable efforts have been devoted to addressing these complications within a broadly Lockean framework \citep[see e.g.,][]{Speaks:2014}.} Regardless of how the details get spelled out, the strategy is clear: first, one tells a story about representational states; then, one tells a story about how language makes it possible to expose these private states to public scrutiny. 

In order for this strategy to work, it must be possible to provide a completely non-linguistic theory of the mental representations that get paired with particular words and sentences upon the learning of a language. Developing such a theory is a challenge, to say the least. One problem is that linguistic expressions tend to come in packages, in the sense that the meaning of one expression cannot be specified without invoking the meanings of others. But if so, then any representation that gets transported by some linguistic expression must bear relations to \textit{other} linguistic expressions, in which case the representation cannot be defined in a purely non-linguistic manner. Another problem is that, for many cases, it is plausible that one could not acquire the mental representation corresponding to a particular linguistic expression without first participating in a linguistic practice. Words like ``garrulous'' and ``persuasive'' are a case in point. A third problem is that meanings are supposed to explain facts about language use, and such facts primarily concern the social practices of linguistic communities. Thus, the pairing of a mental representations with a linguistic expression is generally insufficient for the purposes of characterizing the expression's meaning. 

In light of these problems, a number of theorists attracted to use-theoretic approaches to semantics have accordingly views on which communication is not the transportation of thought, but rather a kind of ``joint action'' \citep{Clark:1996} or a co-operative activity that involves navigating across two social perspectives \citep{Brandom:1994}. What is shared during communication is then taken to be analogous to what is shared to by two people engaged in a dance \citep{Clark:1996,Brandom:2010}; both act independently, but the significance of each action is determined by the contribution made to the dance as a whole. Similarly, conversationalists speak independently, but the significance of what they say is determined by the contribution made to the conversation as a whole. The point can be made even clearer by again considering the notion of a language game: Just as a making a move with the king in chess depends not just on the king piece but on the nature of chess, so too is it the case that making a move in a language game depends not just on the sentence used to make the move but on the nature of the game itself. As such, there is a lot of ``social stage-setting'' that needs to be in place before it makes sense to talk about sentences meaning particular things \citep[][p. 461]{Brandom:1994}. The importance of this stage-setting cautions against views that tie sentence meanings to mental states in too narrow a fashion. 

One reason to be wary of the approach under consideration is that it makes for a complicated relationship between language and cognition. Nonetheless, I think it is possible to provide a fully adequate account of this relationship while at the same subscribing to the view that communication is kind of a joint action rather than a kind of thought transportation. The key to developing such an account lies in expanding on the sort of strategy advocated by Dennett in the above epigraph. Systems qualify as cognitive insofar as they can be interpreted via the intentional stance, which is a fundamentally linguistic stance. The argument in favor of this view of intentional interpretation is simple: there is no reason to describe the states of a system in representational rather than non-representational terms \textit{unless} one is committed to the applicability of the sort of ``linguistic calculus'' that is implicit in the use of intentional state attributions. In other words, if there is no predictive power to be gained by treating something as though it ``thinks'' or ``understands'' things, then there is no point in attributing representations to it. For instance, one can describe many physical systems in purely mechanical, biological, or chemical terms and get along just fine. Certain systems, however, are helpfully described in intentional terms for the following reason: the inferential relations that govern linguistic expressions are such that mental state attributions that invoke these expressions are predictive of how the system functions. Typically, such systems are those that function analogously to people in some way (e.g., nonlinguistic creatures, intelligent software, robots, etc.), but not always \citep{Hochstein:2011}. The typical cases are unsurprising, since the fact that the intentional stance works for us makes it likely that it will work for other things insofar as they are similar to us.

My goal in this chapter is to argue that an important lesson can be drawn from the role that the intentional stance plays in legitimizing ascriptions of sub-personal representational states. Put simply, one has to \textit{start} with intentional interpretation when theorizing about mental representations, which means that one cannot use these representations to indepedently \textit{derive} an account of how intentional interpretation works. As such, the order of intentional explanation is importantly ``top-down,'' in the sense that the use of intentional vocabulary is fundamentally rooted in the interpretation of social practices. The point of using representational rather than, say, causal descriptions of a system's internal states is to ``hook up'' these descriptions to the implicit calculus of intentional systems theory. The theory therefore has to come first, contrary to what many philosophers seem to assume \citep[e.g.,][]{Millikan:1989,Fodor:1998}.\footnote{All of this is not to make the obviously false claim that there was language before there was the cognition in non-linguistic creatures. Rather, the point is that it only makes sense to describe the behavior of these creatures in semantic terms by ``stretching'' the typical uses of semantic vocabularly in linguistic contexts \citep[][p. 59]{Dennett:2010,Brandom:2010}. As a point of comparison, notice that the description of an electron as a particle that orbits an atom's nucleus involves a similar kind of ``stretching'' -- what an electron does is only loosely analogous to what we typically think of as orbiting. Moreover, if the behavior of electrons were all we had to go on, it is unlikely that we could even identify or introduce the concept of orbiting on this basis behavior.}

This thesis concerning the priority of intentional interpretation has some interesting implications for debates about cognitive architecture. Intentional interpretation arises from language use, and to be a competent language user, one must be able to make certain predictions and draw certain inferences. More generally, one must be able to \textit{do} certain things by acting in particular ways. This suggests that cognitive systems are organized so as to satisfy certain function specifications. Recent accounts of cognitive architecture that are organized around the idea that brains function to control behavior \citep[e.g.,][]{Eliasmith:2003} accordingly fit naturally with a usage-based, inferentialist approach to semantics. As such, I contend that the inferential role semantics I have developed in no way violates the cognitive plausibility criterion introduced in Chapter 1.  

The rest of this chapter is divided into three sections. The first section develops the idea that it only makes sense to attribute representations to things that can be interpreted as intentional systems, or things that satisfy function specifications described from the perspective of the intentional stance. The second section develops the idea that the linguistic calculus that constitutes intentional interpretation derives from the social nature of language use. The third and final section develops the idea that intentional systems -- the houses of mental representations -- are systems that \textit{implement} the particular behaviors and functions that warrant identifying them as intentional in the first place.    

Before proceeding, it is worth deflecting a potential misunderstanding. I am in no way claiming that the folk psychological categories of intentional systems theory have a privileged status in the scientific study of the mind. Rather, I take these categories to be intrinsic to the phenomenon of language use, since speakers interpret one another as agents that understand (or misunderstand) things that are said.\footnote{It is helpful to notice that ``understands'', like ``wants'' or ``thinks'', is a clear example of a propositional attitude verb of the sort used in intentional state attributions.} As such, explaining language use requires explaining \text{how} intentional interpretation works. Overall, insofar as folk psychology plays a role in the account developed here, it acts as a phenomenon in need of explanation and \textit{not} as source of explanatory insight. 

\section{Rethinking Mental Representation}

While it widely assumed that mental representations are naturalistic entities, efforts to naturalize mental representation have not exactly been paradigms of theoretical success \citep[][Ch. 3]{Horwich:2005}. The basic problem is that representations are subject to assessments of error, and what counts as an error or instance of \textit{misrepresentation} cannot be ``read off'' of a physical description of a system in which representations are hypothesized to reside. A result of this problem is that naturalistic accounts of mental representation tend to either (a) fail in usefully distinguishing things that are representations from things that are not, or (b) smuggle in normativity at some point. By examining how accounts of mental representation tend to fall to one side or the other of this divide, I will illustrate why ascribing representations to a system only makes sense if it is possible to interpret the system using the intentional strategy. 

\subsection{Representation Attributions are Normative}

The first side of the divide confronts theories that appeal to causal, informational, or statistical relations as criteria for demarcating representations from non-representations. To explain, it is widely acknowledged that the fact that occurrences of a state are caused by occurrences of particular objects does not imply that the former represent the latter, since causes are everywhere, while representations are not. But making the causal story more complicated by invoking asymmetrical \citep[][Ch. 4]{Fodor:1987} or statistical \citep{Eliasmith:2000} dependencies amongst causes does little to improve things, since such dependencies are also everywhere. For instance, the growth of moss on a tree trunk is asymmetrically dependent on the presence of sunlight in comparison to lamplight (in the sense that the moss grows in lamplight because it grows in sunlight, but not vice versa), but moss is not by this token a representation of sunlight.\footnote{Moss might very well indicate sunlight to \textit{us} in certain circumstances, but this fact is irrelevant to the present discussion.} Similarly, a dead fire is highly statistically dependent on (and can be caused by) a lack of fuel, but a dead fire does not represent the world as lacking fuel. Given as much, the idea that representations can be ``read off'' of naturalistic relations characterized in purely causal or statistical terms is a non-starter. 

The second side of the divide confronts theories that appeal to the ``proper function" of mechanisms that produce and consume representational states \citep{Millikan:1989,Millikan:2005,Dretske:1986}, or the ``functional roles" of representational states \citep{Block:1986,Harman:1982,Eliasmith:2000}. The problem here is that specifying the role of a state or the function of a mechanism is tantamount to specifying how it \textit{ought} to behave. Functional accounts of representation are therefore inherently normative \citep{Brandom:1994}. To illustrate with an example, Millikan (\citeyear{Millikan:1989}) claims that cognitive systems can be decomposed into systems that produce representations and systems that consume representations. A state then counts as a representation on this model if two conditions are met (pp. 287-288): first, the systems that consume the state require it to correspond to the world in a certain way in order to function properly, and second, the state must vary so as to represent different things to consuming systems in different situations. For example, the states produced in a frog's visual system by the presence of small bug-shaped dots count as representations of bugs on Millikan's view because they trigger a tongue reflex whose proper function is to result in bug ingestion (p. 291). If the visual states in question do not correspond to the presence of bugs (and vary in accordance with the position and velocity of these bugs), then the frog's behavior will fail to result in bug ingestion, which is contrary to its purpose. Ingesting flies is what the frog \textit{ought} to do, and the fact that this purpose is not furthered in cases where the frog snatches up bug-shaped wooden pellets is what warrants treating such cases as instances of misrepresentation or error. The key point in all of this is that it is only possible to read representations off of naturalistic descriptions of a cognitive systems if one makes further assumptions about the design, purpose, or function or these systems. Such assumptions are clearly normative, not descriptive.

One lesson to draw from this discussion is that the question of whether a system has a certain representation is quite different from the question of whether a system has, say, a certain mass or volume. One need not assume anything further about a system in order to determine its mass or volume; one simply measures it. But in order to determine whether a system contains a representation of, for instance, the surrounding temperature, one must assume that the system is an agent of sorts, with interests and needs. Put another way, one must adopt the intentional stance and treat the system as though it can know that the temperature is \textit{X}, and can act on this knowledge to warm up or cool off as desired. The cost of not adopting the stance in this case is that any grounds for attributing a temperature representation to an organism (e.g., by identifying some internal state that co-varies with temperature) would also suffice for attributing a similar temperature representation to mundane objects such as copper rings and clay pots. As such, if one cannot treat a system as an intentional system, then there is just no point in attributing representations to it. One is better off attributing non-semantic properties of sort cataloged by biologists, chemists, and physicists.  

Another way to draw the same conclusion is to observe, as others have \citep[e.g.,][]{Brandom:1994}, that representations only represent to or for interpreters of some sort. For example, to say that a danger sign at a construction site is a representation of danger involves assuming that is representation \textit{for} nearby pedestrians. The sign represents \textit{to} the pedestrians that danger is nearby. Similarly, to say that some state in a frog's visual system is a representation of a bug involves assuming that it is a representation for the frog. The state represents to the frog that a bug is in such-and-such a location. Given as much, it is incoherent to attribute a representation to a system without simultaneously treating the system as something that can interpret or understand representations. It is accordingly incoherent to attribute a representation to system without simultaneously treating that system \textit{as an intentional system}. 

The fact that the attribution of ``sub-personal'' mental representations goes hand-in-hand with intentional interpretation suggests that the grounds for attributing such representations are similar to the grounds for attributing a particular collection of linguistically specified intentional states. Put simply, predictive adequacy adjudicates between competing semantic descriptions at both the highest and lowest levels of description in cognitive systems. For example, it is appropriate to treat the frog's neural state as a bug representation rather than a black-dot representation if doing so yields a better overall predictive model of the frog's behavior. Similarly, it is appropriate to describe a stockbroker as believing that shares in IBM are about to drop if doing so yields good predictive model of the broker's behavior \citep[][pp. 24-26]{Dennett:1987}. So, to somewhat extend Dennett's (\citeyear{Dennett:1987}) account, the question of whether or not the frog's visual system has a sub-personal bug representation comes down to whether or not it can be predictively attributed this representation, just as the question of whether or not a stockbroker has a particular belief comes down to whether or not they can be predictively attributed this belief. 

One thing that is importantly unique about low-level semantic interpretation on this view is that insofar as we have greater knowledge about the causal regularities at play in some subsystem, there are greater constraints on what a representational description of this subsystem has to be like in order to be predictively adequate.\footnote{Thanks to Eric Hochstein for helpful discussion on this point and related material concerning teleo-semantics.} This point can be nicely illustrated by once again comparing the frog and the stockbroker. In the case of the frog, the options for a representational characterization of the neural responses in its visual system are narrowly constrained by our detailed experimental knowledge of the kinds of stimuli that elicit these responses; not just any representation attribution will do. In the case of the stockbroker, however, there is much more interpretive slack. One might attribute a belief that the whole market is going to crash, a belief that the tech sector alone is going to crash, or a belief that IBM is about to announce weak earnings, and get equally good predictions about what the broker goes on to do next. In order to rule out some of these hypotheses, one simply needs to learn more about the broker. For instance, one might check to see whether they sell stocks outside of the technology sector to rule out a belief in a market crash. The issue here is that people are vastly more complicated than frog visual systems, which makes it vastly more difficult to find adequate and uniquely predictive intentional interpretations of them. 

\subsection{Representation Attributions are Analogical}

If this general approach to mental representation is on the right track, there is still no obvious trouble for the Lockean view on which the semantics of natural language is parasitic on the semantics of mental representation. I've merely argued that one has to start with the assumption that a system is an intentional system before one can go on to make sense of its internal states as representations. If what counts as being an intentional system - a system for which the intentional strategy works - can then be characterized without appeal to language, the Lockean strategy remains viable. In one sense, there is nothing to wonder about here. It is just obvious that there are non-linguistic intentional systems, and that these systems were around, evolutionarily speaking, long before there were linguistic intentional systems \citep{Dennett:2010,Brandom:1994}. But in another sense, it is not clear that the cognitive capacities exhibited by non-linguistic creatures are at all intentional except insofar as they are analogous to our own, linguistically sophisticated cognitive capacities \citep{Brandom:1994,Brandom:2010}. The intentional stance is a thoroughly \textit{linguistic} stance, and to treat a non-linguistic creature as an intentional system is to adapt our language as a sort of ``measurement system'' for reasoning about the creature \citep[][p. 306]{Brandom:2010}. For instance, treating a neural state in the frog as a representation of a bug involves treating the state as a message to be received or understood by the frog, akin to how the utterance ``there's a bug'' can act as a message to be received or understood by a person. Moreover, it is also clear that frogs do not represent bugs \textit{as} bugs in the same way that people do. The application of intentional terminology to frogs and other non-linguistic creatures is therefore importantly analogical, in that we treat these creatures \textit{as though} they can think and reason about things like bugs in the same way that language users do.

One advantage of this view is that it helps make sense of where and why the intentional stance both fails and succeeds. First, consider a simple artifact such as a clothespin. One could characterize a clothespin as capable of thinking that its prongs are either touching or not touching, and as always wanting for them be touching. One could accordingly use the intentional stance to predict the clothespin's behavior - any time its prongs are forced apart, the clothespin notices, and tries to force them back together. This characterization is pointless for the simple reason that everything the pin does can be described simply and non-intentionally in terms of the force exerted by a spring in relation to angle of the prongs. Now consider Drestke's (\citeyear{Dretske:1986}) example of an ocean-dwelling bacterium that ``represents'' the location of oxygenated surface water (which is toxic to it) through a mechanism that is, in relative terms, only somewhat more complicated than a clothespin \citep[cf.][p. 290-91]{Millikan:1989}. The bacterium contains an internal ``magnetosome'' that draws it directly towards magnetic north, which in the northern hemisphere is downwards and away from the toxic surface water (in the southern hemisphere, the magnetosome is reversed). An intentional interpretation of the bacterium is potentially useful because there is clearly a sense in which it ``wants'' to stay away from the toxic surface water; if it doesn't stay away, it will die. Moreover, it becomes possible to make sense of potential errors on the part of the bacterium, as when it is fooled by a nearby magnet. The appropriateness of this kind of intentional interpretation is questionable, though, given that we can predict most of what happens to the bacterium by appealing solely to the effects of magnetic forces. What we can't predict in terms of such effects are the details of how oxygen destroys the bacterium. If these details are not fully understood, positing a need to avoid oxygen on the part of the bacterium is an effective tool for making sense of its behavior. On the other hand, if the details are simple, the bacterium can be treated like a complicated clothespin with no loss of explanatory power. 

Does the bacterium \textit{really} represent its environment using its magnetosome? It's doubtful that an answer to this question can serve any purpose other than satisfying the prejudices of intuition. The important thing is that intentional interpretation is more predictively useful for complicated systems that are difficult to understand in purely biological or mechanical terms. Consider an ordinary rat. A considerable amount of neuroscientific research has been directed towards understanding the neural mechanisms by which rats come to learn to navigate through complicated spatial environments. One of most prominent findings of this research concerns the existence of ``place cells'' \citep[][p. 369]{Eliasmith:2013}, or neurons that selectively respond to particular locations in the environment. The responses of these neurons are accordingly characterized as representations that serve as a map of sorts to direct the rat's movements. Intentional interpretation is valuable here because it is possible to predict to what a rat is going to do if it ``thinks'' it is in such-and-such location and ``wants'' the treat located to the left of this location rather than the right. Predicting that the rat will go left is easy if one adopts the intentional stance, and probably very hard if one tries to reason through a causal chain of events in the rat's nervous system. To make the utility of the stance even clearer, consider a person who says ``I'm arriving in China tomorrow evening'' after being asked about their travel schedule. Intentional interpretation is at this point indispensable, as it offers pretty much the \textit{only} way to make a reliable prediction about which country the person will be in tomorrow evening. 

The point of comparing clothespins and bacteria to rats and people is to illustrate that there is a broad spectrum of applicability for the intentional stance. Adopting the stance works insofar as the target of interpretation is analogous to us --- the creatures that made the stance visible in the first place through our efforts to interpret one another. Overall, representational states are only found in systems well-described by adopting the intentional stance, and the intentional stance itself is a product of our own linguistic practices, arising from the fact that we treat one another as thinkers and knowers in virtue of being speakers. It is accordingly plausible that intentional interpretation is modeled on and therefore presupposes public language use. Providing a more detailed argument in favor of this claim is the purpose of the next section. 

\section{The Social Origins of Intentional Interpretation}

The idea that intentional interpretation only gets off the ground in the presence of socially instituted norms governing language use is largely at odds with conventional philosophical wisdom. The standard view, again, is that we are thinkers first and speakers second, or that non-linguistic representations are explanatorily prior to linguistic representations. In the context of intentional systems theory, however, a departure from conventional wisdom is well motivated. To see why, notice that while the theory provides criteria for identifying intentional systems in terms of the predictive successes of intentional interpreters, it does \textit{not} provide criteria for identifying intentional interpreters \citep[][p. 57-59]{Brandom:1994}. This state of affairs amounts to solving one mystery by trafficking in another: what makes it possible to adopt the intentional stance in the first place?

An answer to this question can be given through appeal to public linguistic practices. To adopt the intentional stance, one must be a language user, and to be a language user, one must internalize the norms of a linguistic community. Internalizing these norms, in the first instance, involves learning how to properly use particular linguistic expressions in particular interactions, such as when a child learns to name or retrieve a toy for an adult \citep[][]{Tomasello:2005}. More generally, learning a language involves learning about what follows from one's own verbal behavior and about what follows from the verbal behavior of others. If I ask you a question, I can usually expect an answer of some sort. If you tell me not to take your soda from the fridge, I can expect you to get angry if I do. If I describe a candle as a broomstick, I can expect anyone listening to me to be confused. And so on. The point is that these expectations are all ones that we \textit{confer on each another}; trivially, they arise from communal interaction.

Expectations involving the use of mental state verbs such as ``thinks'', ``understands'', and ``wants'' are no exception to this general rule. One learns about these terms by learning the ``conventions of use and response'' \citep[][p. 50]{Millikan:2005} that govern their behavior in the linguistic community. For instance, if James is described as  thinking that the stuffed bear is in the toy box, one learns to expect that if James wants the bear, he will look in the toy box (rather than, say, in the laundry hamper or under the coffee table), even if the bear is not actually in the toy box. It is quite uncontroversial from the perspective of developmental psychology that ``theory of mind'', or the ability to engage to intentional interpretation, emerges in tandem with linguistic ability \citep{Miller:2006}. For instance, children typically develop an understanding of mental state terms via linguistic explanation, and their performance on false-belief tasks is markedly improved when they are instructed in the use of certain linguistic constructions. Moreover, their initial linguistic abilities are much more predictive of their later theory of mind abilities than vice versa. This asymmetry suggests that the ability to use language is an important precursor to the ability to engage in intentional interpretation. Overall, regardless of the exact nature of the relationship between language use and theory of mind, it is quite clear that the two tend to go hand-in-hand.

Some reasons for this connection between language use and intentional interpretation are fairly easy to identify. On an empirical level, language learning involves forming expectations about the roles played by certain noises and gestures in social interaction \citep{Tomasello:2005}. It also involves identifying others as the primary producers and consumers of these noises and gestures; one's expectations become attuned to the fact that the consequences of language use always get routed through people. As such, the need to keep track of these highly complex consequences creates pressure to interpret or model the people from which they originate. Adopting the implicit calculus of intentional interpretation is a natural way of relieving this pressure. On a more conceptual level, noises and gestures only \textit{become} meaningful by being \textit{treated} as meaningful within a community of language users \citep{Brandom:1994}. And as noises and gestures become meaningful, the beings that produce and respond to them become beings that \textit{mean} by speaking and \textit{understand} by listening. The fact that meaning and understanding are ``two sides of the same coin'' in this way illustrates the conceptual dependence between intentional interpretation and language use. Put simply, a precondition of meaning something is knowing what would constitute someone else \textit{understanding} what was meant, and treating someone else as potentially capable of understanding trivially involves adopting the intentional stance towards them. As such, the idea that a person can genuinely mean something by their behavior without being able to engage in intentional interpretation is incoherent \citep{Brandom:1994}. Overall, the thesis that intentional interpretation has social origins amounts to the claim that by conferring expectations upon one another via their joint linguistic behavior, people \textit{mold} one another into intentional interpreters. 

A well-known concern with this approach is that is it treats intentionality as a primarily \textit{linguistic} phenomenon, which misleadingly ignores the more typical cases of intentional phenomena in non-linguistic creatures \citep{Eliasmith:2000,Dennett:1987,Dennett:2010,Millikan:1989,Millikan:2005,Dretske:1986}. Language using creatures are merely the most complex and sophisticated representational systems, so it makes sense to try to understand simpler representational systems first. Indeed, absent some story about how simpler representational systems transitioned into more complicated systems that exhibit linguistic abilities, the existence of language (and hence intentionality) is left to appear almost magical \citep{Dennett:2010}. 

Proponents of this criticism must, of course, provide an alternative account that makes no appeal to norms of public language while nonetheless underwriting norms of representational correctness. A typical way of discharging this responsibility involves an appeal to evolutionary considerations \citep{Millikan:1989,Millikan:2005,Dennett:1987,Dennett:2010}. The idea is that various mechanisms in non-linguistic organisms are ``designed'' via natural selection to indicate or detect certain features of the environment, and thereby form representations. For instance, the famous ``dance of the honeybee'' has the function of indicating the location of nectar to other bees, and the fact that the dance has this function is because it has evolved to have this function \citep[see][]{Sellars:1953}. As Millikan (\citeyear{Millikan:1989}) puts it, the ``proper function'' of a process such as the honeybee dance is whatever explains or is responsible for the process's continued existence in the honeybee population. Dennett (\citeyear{Dennett:1987}, Ch. 8) provides a nice analogy that generalizes this idea further. Organisms can be thought of ``survival machines'' that acquire capacities to monitor and react to their environment only insofar as these capacities help preserve their existence from one generation to the next. Natural selection, through a random search of possible machine designs, settles on a design that constitutes a minimal solution to the problem of survival. The ``specs'' of this design thus constitute the norms against which instances of misrepresentation and error can be evaluated. 

There are three general problems with this teleo-functional approach to intentionality. First of all, the idea that natural selection is the source of all intentional phenomena is both trivially true and largely uninformative. It is trivially true because no reasonable person would disagree with the claim that cognitive beings have come to exist they do as a result of natural selection. But to say as much is uninformative because no explanation is provided for why certain states and mechanisms in organisms should be described in representational terms rather than in causal or biological terms. In fact, the idea that a state has the function of indicating or detecting already presupposes the idea that the state resides in an intentional system. Moreover, treating nature as a ``designer'' (even if just metaphorically) that assigns functions to various representation-producing mechanisms \textit{assumes} the intentional stance as a tool for interpreting evolutionary processes. One needs to indulge in the pretense that nature is a purposive agent in order to make sense of the idea that nature designs or determines the ``proper function'' of various biological mechanisms. But helping oneself to the intentional stance in an explanation of the origins of intentionality is blatantly circular. 

A second problem is that even if a non-circular teleological account of intentionality can be given, it is not at all obvious how such an account might be extended the full range of intentional phenomena in language users. All parties to the debate acknowledge that selectional pressures do not yield mechanisms for producing particular representational states of the sort corresponding to sentences like ``The election results were surprising'' or ``There's the leader of the tribe''. Rather, the story is that organisms have adapted to have generic representation-producing mechanisms that can be used in highly general and ``innumerably diverse'' ways \citep[][p. 292]{Millikan:1989}. True enough, but identifying evolutionary pressures as distal causes of intentional phenomena is a bit like identifying the existence of carbon as a distal cause of certain architectural phenomena. Learning about carbon won't tell you much about the architecture of buildings made from carbon-based metals, just as evolutionary considerations won't tell you much about meanings of specific linguistic expressions. In any event, teleological accounts of intentionality are typically only worked out for cases involving low-level perceivables such as object locations \citep{Millikan:1989}, or mid-level perceivables such as faces \citep{Dennett:1987}. Building up towards mental representations of more complicated things is, to put it mildly, a challenge. The fact that this challenge has not been met despite decades of effort suggests that it is wise to try a different approach.

A final problem is that mental representations are simply not well-defined theoretical entities. This point can be illustrated by comparing representations to other objects of scientific study such as nucleotides and electrons. In the case of these latter objects, there are clear standards for discriminating them from other things (e.g., phosphates, neutrons) on the basis of their characteristic properties and the phenomena they give rise to. Mental representations, by contrast, have no agreed upon defining characteristics, except perhaps that they ``play a role in cognition'' and ``correspond to things in the world''. This lack of a cohesive characterization is evident in the recent cognitive science literature, in which representations are variously described as symbols \citep{Fodor:1998}, emulators \citep{Grush:2004}, simulators \citep{Barsalou:1999}, patterns of activation \citep{McClelland:2010}, probability distributions \citep{Goodman:2015} , control-theoretic state variables \citep{Eliasmith:2003,Eliasmith:2013}, and various other things. It is doubtful that any other ostensibly naturalistic entity has been described in so many competing ways. Moreover, such a lack of clarity concerning what representations are and do provides further evidence for the idea that talk of mental representation is a kind of analogical projection, or a mapping of intentional interpretation from linguistic contexts onto contexts in which the interpretation of sub-personal cognitive machinery is at issue.

To be clear, the conclusion here is \textit{not} that one should adopt a linguistically oriented approach to studying cognitive systems, or even that mental representations are inherently linguistic. Rather, the conclusion is that the use of semantic vocabulary in characterizing the internal states of cognitive systems ought to be seen as a way of hooking these characterizations up to the implicit calculus of intentional systems theory. For instance, designating a state as a representational state as opposed to a solely neural or psychological state introduces a commitment to the idea that the surrounding system \textit{makes use} of the state in a purposive manner, analogously to how people make use of things that are said to them. 

\section{Concerns with Cognitive Architecture}

If being a language user is a matter of satisfying a certain function specification, then not just anything can \textit{be} a language user. Keeping track of of the defeasible expectations that regulate communal language use requires the possession of certain information-processing capabilities. Since brains are principally responsible for the realization of such information-processing capabilities, the following two questions naturally arise. First, what kinds of things do brains have to be able to do in order implement the kind of intentional interpretation that gives rise to meaningful language use? And second, do brains actually do these things? Call these the functional and implementational questions, respectively. Answering them is the purpose of the remainder of this section. 

Concerning the functional question, a useful starting point is the observation that brains seem to be largely in the business of performing statistical inference \citep{Eliasmith:2007,clark:2013}. This is good news, since keeping linguistic score on others involves forming probabilistic expectations about they are likely do next during a conversation. The main constitutive requirements of scorekeeping are accordingly that one (a) be able to \textit{infer} the likely consequences of various forms of linguistic behavior, and (b) be able to \text{act} appropriately on the basis of these inferred consequences. The first of these requirements concerns language comprehension, while the second concerns language production. 

To introduce a bit more in the way of details, it helps to consider an intuitive example of the sort described in Chapter 1. Suppose Jim and Jane are talking about Jane's recent hiking trip. Jim asks, ``Did you see any interesting animals on the hike?'' To count as properly understanding this question, it is plausible that Jane has to be able to form certain predictions about what Jim is likely to do next. In the absence of some expectation that Jim will be taken aback if she does not respond, it is hard to justify the claim that Jane understands that Jim \textit{asked a question}. Similarly, in the absence of some expectation that Jim will puzzled if she responds by talking about how she is tired, it is hard to justify the claim that Jane understands that Jim's question was about whether or not she \textit{saw animals}. Now suppose that Jane's actual response is ``Yeah, I saw a badger leaving its burrow at one point.'' To count as properly understanding this response, Jim has to likewise form certain predictions about what Jane is likely to do next. For instance, Jim ought to expect that Jane will acknowledge having seen a gray animal with stripes. Jim also ought to expect that Jane will naturally answer certain further questions such as ``How big was it?'' or ``How far away was it?'', but not ``How how high in the air was it?'' (or at least not without some consternation). And whatever Jim's actual response is, it is fair to assume that it will induce yet further expectations in Jane insofar as she is capable of understanding what was said. 

The point of the example is to illustrate that one can only \textit{use} language in an effective manner if one has a model of the expected consequences of linguistic acts. Otherwise, dialogue breaks down under the weight of odd silences and frequent non-sequiters. Just imagine if Jane's response to Jim's initial question was ``I'm oddly tired today'', while Jim's follow-up was ``How high in the air was it?'' Models of the inferential roles of linguistic expressions, again, are hypothesized to be just the sort of models we use avoid such conversational mishaps. For instance, the reason that Jim can expect Jane to answer questions concerning the size and shape of the badger (but not its altitude) is because it follows from fact that something is a badger that it has a size, and that it can be nearer or farther away. The inferential role of the sentence ``I saw a badger leaving its burrow at one point'' accordingly specifies the targets of plausible questions and hence the plausible directions in which the conversation can proceed. Likewise, the fact that there is no straightforward inference from something being a badger to it being airborne explains why questions concerning altitude are so out of place.

Altogether, keeping score on a conversation requires keeping track of what \textit{can be} said as a result of what \textit{has been} said. The fact that linguistic expressions bear inferential relationships towards one another is what makes such scorekeeping possible. In other words, a sentence's inferential role is what determines \textit{how} the conversational score is altered when the sentence is uttered, by settling which further sentences can plausibly be uttered by each interlocutor. This general point can be usefully illustrated by observing that each move in a conversational game is effectively an inference from what was just said to what is said next. For instance, providing an answer to a question clearly involves an inference whose premises are the question itself and various further facts known to the answerer. Asking a question similarly involves an inference from some prior statements to a request for further relevant information. For instance, if these prior statements concern the value of an antique vase, then there is no plausible inference from these statements to a question about, say, the distance between Waterloo and Toronto.  

The preceding description is a slight oversimplification given that pragmatic considerations involving such things as the social roles, appropriate subject matter, and general etiquette also influence how questions are asked and answered. But to admit this in no way undermines the claim that inferential roles are explanatorily fundamental to the keeping of conversational score. Taking etiquette into account, for instance, presupposes that there are more basic conversational principles that the norms of etiquette somehow infringe upon. Shifts in subject matter similarly presuppose that within-subject dialogue is independently well-defined. Separating out the core regularities of language use from more peripheral and context-dependent ones is undoubtedly hard work, but then again, linguistic phenomena are some of the most complex phenomena in existence, so the need for hard work should come as no surprise.

Now that it is clear that the minimal requirements of scorekeeping involve being able to predict the consequences of various linguistic acts, it is possible to turn to the implementational question: Do brains keep score by predicting and inferring upcoming conversational events? A good starting point for answering this question can be found in recent research concerning the role of prediction in language processing \citep{Pickering:2013,Pickering:2007,Christiansen:2015}, and cognition more generally \citep{clark:2013}. I consider three sources of evidence indicating that the model I've outlined is consistent with our current understanding of linguistic cognition.

The first source of evidence concerns conversational turn-taking. Effective dialogue requires participants to alternate between speaking and listening in an appropriate manner, and to avoid wasteful lags between such alternations, people implicitly predict when and how a speaker will stop talking \citep{Christiansen:2015,Pickering:2013}. It is clear that such anticipation occurs given that shifts between speakers are nearly instantaneous, which means that listeners begin to generate a response \textit{before} the speaker even finishes, given that the syntactic, phonological, and articulatory processes involved in speech production collectively take several hundred milliseconds to complete \citep{Pickering:2013}.\footnote{Notice too that the predictions underlying such rapid alternations cannot merely be along the lines of ``It's my turn now'', given that the anticipatory initiation of speech production has to result in a very specific utterance. The natural conclusion to draw here is that listeners make use of prediction to start responding to exactly what speakers say before they even finish saying it.} Further evidence of this kind of anticipation can be found in fMRI studies that indicate a general alignment in the patterns of neural activity observed across speakers and listeners, with speakers lagging behind listeners in striatal and anterior frontal areas in virtue of listeners trying to anticipate the current trajectory of the dialogue \citep[reviewed in][]{Christiansen:2015}.  Such alignment between speakers and listeners also appears to strongly facilitate mutual comprehension \citep{Pickering:2013,Christiansen:2015}. This all makes sense on my account, since understanding an utterance is taken to involve inferring what follows from it. 

A second source of evidence concerns the incremental interpretation of speech. As mentioned in Chapter 1, the fact that speech is voluminous while memory is limited favors processing mechanisms that rapidly compress linguistic inputs into increasingly abstract ``chunks'' \citep{Christiansen:2015}. To facilitate such chunking, predictions of subsequent inputs are used to constrain how current inputs are compressed, so as to quickly resolve local ambiguities and avoid having to backtrack later on (p. 9). Evidence of the existence of such anticipatory processing is perhaps most pronounced in the literature on the N400 event-related potential \citep{Federmeier:2007,Kutas:2011}. This response is evoked by the presentation of anomalous sentence continuations (e.g., ``He planted beans in his \underline{car}''), and is hypothesized to reflect processing involved in suppressing or revising an earlier prediction of more plausible continuations (e.g., ``He planted beans in his \underline{garden}'', see p. 2). Eye-tracking studies similarly indicate that subjects attend to particular visible objects in anticipation of sentence continuations. For instance, subjects presented with an image depicting a young girl, a carousel, and a motorcycle will fixate on the carousel upon hearing, ``The girl will ride the...'' \citep[][p. 194]{Kutas:2009}. Overall, there is little doubt that people understand speech by incrementally building up a set of expectations about what the speaker is likely to say next. 

A third source of evidence concerns the existence of discourse-level predictions. Studies of narrative processing, for example, indicate that subjects anticipate specific sentential transitions on the basis of a variety of pragmatic cues \citep{Rohde:2008}. The effect of these cues is to suggest a particular kind of an inferential transition from one sentence to the next. For instance, sentences describing ongoing events (e.g., ``John was handing a book to Bob'') tend to generate expectations of transitions that are either explanatory (e.g., ``He wanted Bob to read it'') or elaborative (e.g., ``He reached across the table''). Sentences describing completed events (e.g., ``John handed a book to Bob'', in contrast, tend to generate expectations of transitions that introduce new information (e.g., ``He took it and thanked John''). The existence of these expectations is revealed in part by how people interpret pronouns. Specifically, in the case of a statement describing an ongoing event, the initial pronoun in a subsequent statement tends to be immediately interpreted as referring to the individual associated with the \textit{start} of the event (i.e., ``John'' in the example above). In the case of a statement describing a completed event, however, the initial pronoun in a subsequent statement tends to be immediately interpreted as referring to the individual associated with the \textit{result} of the event (i.e., ``Bob'' in the example above). This difference is hard to explain unless subjects are relying on their tacit expectations of how the narrative will continue to preemptively settle on a particular interpretation. It is also possible to bias the interpretation in question by manipulating the prior narrative to suggest the onset of a particular type of inferential transition \citep{Rohde:2008}. Altogether, this research provides quite direct evidence in favor of my theory: ``keeping score'' on a speaker just amounts to updating a set of expectations about what they are likely to say and do next.\footnote{The fact that the inference from ``John was handing a book to Bob'' to ``John wanted Bob to read it'' is highly contingent on further facts about John and Bob does not undermine the claim that the inference is partially constitutive of what these sentences mean. To explain, if it were equally plausible to reason from ``John was handing a book to Bob'' to either ``John wanted Bob to read it'' or ``Woodworking is a common hobby'', then ``John was handing a book to Bob'' would clearly not mean what it in fact does mean. Moreover, inferences of the sort in question typically involve multiple (perhaps implicit) premises that collectively lend plausibility to the conclusion.}

There are two caveats to keep in mind with regard to the question of how all of this evidence bears on the model described in Chapter 3. First, the model does not directly account for any of the empirical phenomena just discussed, with the exception of discourse transitions that involve elaboration, since predicting an elaboration involves anticipating what is entailed by a given statement. As such, there remains a lot of work to do in identifying the mechanisms and processes by which speakers exploit their tacit understanding of the inferential roles of sentences to keep score on one another. Second, I have not directly made use of existing approaches to modeling the kinds of prediction just discussed. For instance, work on ``predictive processing'' is typically carried out by assuming that a generative model matches top-down sensory predictions to bottom-up sensory information, and that sensory representations are in fact representations of prediction-error \citep{clark:2013}.\footnote{These representations of error are then hypothesized drive both learning and behavior so as to reduce the error.} I do not take the commitments and details of my model to be inconsistent with this approach, and welcome further work that involves it.

\section{Conclusion}

What, then, is the nature of the relationship between language and cognition? Meaning, on the view proposed here, is very much a part of what Wilfrid Sellars (\citeyear{Sellars:1963}) calls ``the manifest image,'' or the intuitive theories that make up our commonsense understanding of the world. The notion that one thing can be a representation of another only becomes visible in communal, normative practices of intentional interpretation; once these practices are in place, it then becomes possible to analogically apply the notion of something being a representation to make sense of non-linguistic mental states. There are, of course, a variety of facts about people that make it such that we interact in the ways that we do and thereby come to conceive of the world in the ways that we do. These facts are of the sort analyzed and described by biologists and neuroscientists in largely non-semantic terms. The introduction of semantic vocabulary is accordingly a way of analogically using the commonsense notions of representation and meaning to explain how we are built to acquire these notions in the first place. As such, making states and processes visible as representations amounts to retrospectively interpreting the preconditions of our own linguistic intentionality in terms of their relationship to the kind of cognitive prowess that is their end result.